# -*- coding: utf-8 -*-
"""pyomo.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1T8nQeEgyBt_tIpHIkZ0DkAL1T44B3msz
"""

import pandas as pd;pd.__version__
from google.colab import auth
auth.authenticate_user()

import gspread
from google.auth import default
from datetime import datetime, timedelta

creds, _ = default()
gc = gspread.authorize(creds)

spreadsheet = gc.create('Workout_Scheduler_Template')
print(f"Created: {spreadsheet.url}")

# UserData sheet (first sheet)
user_sheet = spreadsheet.sheet1
user_sheet.update_title('UserData')
user_sheet.update(values=[['user_id', '1']], range_name='A1:B1')
user_sheet.format('A1:B1', {'textFormat': {'bold': True}})

# Schedule sheet
schedule_sheet = spreadsheet.add_worksheet('Schedule', rows=50, cols=5)
schedule_sheet.update(values=[['Date', 'Workout_Sheet_Name', 'Completed', 'Actual_Duration', 'Notes']], range_name='A1:E1')

today = datetime.now()
rows = [[(today + timedelta(days=i)).strftime('%Y-%m-%d'), f'Workout_Day_{i+1}', 'No', '', ''] for i in range(7)]
schedule_sheet.update(values=rows, range_name='A2:E8')

# Workout 1
w1 = spreadsheet.add_worksheet('Workout_Day_1', rows=15, cols=5)
w1.update(values=[['Interval_Order', 'Duration_Sec', 'Pace', 'Cadence', 'Type']], range_name='A1:E1')
w1.update(values=[[1,600,'6:00/km',85,'warmup'],[2,240,'5:00/km',90,'work'],[3,120,'6:30/km',80,'recovery'],[4,240,'5:00/km',90,'work'],[5,120,'6:30/km',80,'recovery'],[6,240,'5:00/km',90,'work'],[7,600,'6:00/km',85,'cooldown']], range_name='A2:E8')
w1.update(values=[['Location: track'],['Music: tempo_mix.mp3'],['Notes: Tempo intervals']], range_name='A10:A12')

# Workout 2
w2 = spreadsheet.add_worksheet('Workout_Day_2', rows=15, cols=5)
w2.update(values=[['Interval_Order', 'Duration_Sec', 'Pace', 'Cadence', 'Type']], range_name='A1:E1')
w2.update(values=[[1,300,'6:00/km',85,'warmup'],[2,1800,'6:00/km',85,'steady'],[3,300,'6:30/km',80,'cooldown']], range_name='A2:E4')
w2.update(values=[['Location: park'],['Music: easy_mix.mp3'],['Notes: Easy run']], range_name='A6:A8')

# Workout 3
w3 = spreadsheet.add_worksheet('Workout_Day_3', rows=15, cols=5)
w3.update(values=[['Interval_Order', 'Duration_Sec', 'Pace', 'Cadence', 'Type']], range_name='A1:E1')
w3.update(values=[[1,600,'6:00/km',85,'warmup'],[2,120,'5:30/km',88,'hill'],[3,120,'7:00/km',75,'recovery'],[4,120,'5:30/km',88,'hill'],[5,120,'7:00/km',75,'recovery'],[6,120,'5:30/km',88,'hill'],[7,600,'6:00/km',85,'cooldown']], range_name='A2:E8')
w3.update(values=[['Location: hills'],['Music: hill_mix.mp3'],['Notes: Hill repeats']], range_name='A10:A12')

print("âœ… Done!")
print(f"ðŸ“‹ URL: {spreadsheet.url}")
print("\nUserData sheet created with user_id=1 (edit B1 to change)")

"""# usor2"""

from google.colab import drive
drive.mount('/content/drive')
import sys
import os
import pandas as pd
sys.path.append('/content/drive/Othercomputers/My Laptop (2)/myproject3')#for import uscvxpy
vd="/content/drive/MyDrive/US/oxalate/mgca1to1/b6/glutamic/homeless/shelterfood/bipolar ratios/arginine/methionine/water/mealcycling/"
vdsols="/content/drive/MyDrive/US/oxalate/mgca1to1/b6/glutamic/homeless/shelterfood/bipolar ratios/arginine/methionine/water/mealcycling/sols/"

os.chdir(vd)#synching with laptop verion"/content/drive/Othercomputers/My Laptop (2)/myproject3")#save cplex.mps and load model.sol
os.getcwd()#,os.listdir()
import time
path = vd+"chosenfoods1.xlsx"
ti_c = os.path.getctime(path)
ti_m = os.path.getmtime(path)
time.ctime(ti_m)

import importlib; importlib.reload(uspyomo)

import uspyomo

uspyomo.vdwin=vd
uspyomo.vd=vd

uspyomo.consdf=None
uspyomo.npt=None

uspyomo.chosenfoods=None

import os
import time

# Function to check if a file was modified within the last N seconds
def is_file_recent(file_path, threshold_seconds=60):
    if not os.path.exists(file_path):
        print("File does not exist.")
        return False

    # Get the current time and file modification time
    current_time = time.time()
    file_mod_time = os.path.getmtime(file_path)

    # Calculate time difference
    time_difference = current_time - file_mod_time

    # Check if the file was modified within the threshold
    if time_difference < threshold_seconds:
        print(f"The file '{file_path}' was modified {time_difference:.2f} seconds ago.")
        return True
    else:
        print(f"The file '{file_path}' was last modified {time_difference:.2f} seconds ago, which exceeds the threshold.")
        return False

# Specify your file path
file_path = vd+'chosenfoods1.xlsx'

# Check if the file was modified within the last 60 seconds
is_file_recent(file_path, threshold_seconds=60)

import pandas as pd
uspyomo.loaddataset()

import pandas as pd
chosenfoods1=pd.read_excel(vd +'chosenfoods1.xlsx',header=0,dtype={'NDB_No':str}).set_index('NDB_No')#index_col=0)#load it up, edited OR NOT
#todo have to carry over the composite foods like this too!!!  and the newfood aminos hardcoded in the module
npt=pd.read_csv(vd+'npt.csv',index_col=None,dtype={'NDB_No':str});npt.set_index('NDB_No',inplace=True)

#not doing this because all the nutrients are going into chosenfoods1
#npt.to_csv(vd+'npt.csv')#comes from dataframe_hacking.ipynb

import pandas as pd
import numpy as np
from openpyxl import load_workbook
from datetime import date

# --- Assumes these are already loaded ---
# npt = pd.read_pickle("npt.pkl")
# chosenfoods1 = pd.read_excel("chosenfoods1.xlsx", index_col=0)

# --- Step 1: Set up food IDs ---
green_cabbage_id = '11109'
green_sauerkraut_id = '11439'
red_cabbage_id = '11112'
red_kraut_id = 'redkraut'

# --- Step 2: Create base from red cabbage ---
rk = npt.loc[red_cabbage_id].astype(float).copy()  # Full baseline nutrient row


# --- Step 3: Apply fermentation transformation to selected nutrients ---
transform_nutrients = ['203', '204', '208', '269', '212', '291', '401']

gc = npt.loc[green_cabbage_id].astype(float)
gs = npt.loc[green_sauerkraut_id].astype(float)

gc_safe = gc.replace(0, np.nan)
ratios = gs[transform_nutrients] / gc_safe[transform_nutrients]

# Apply transformation where possible
for nutr in transform_nutrients:
    if pd.notna(ratios[nutr]) and nutr in rk:
        rk[nutr] = round(rk[nutr] * ratios[nutr], 3)

# --- Step 4: Override sodium based on salt % ---
salt_pct = 1.0  # 1% salt
natural_na = rk.get('307', 0.0)
added_na = salt_pct * 393  # mg per 100g for 1% salt
rk['307'] = round(natural_na + added_na, 2)

rk1= pd.DataFrame(rk).T.assign(
    Long_Desc='Red cabbage sauerkraut (1% salt, inferred)',
    price=0.45,
    date_modified=date.today().isoformat()
).rename({0: red_kraut_id})

chosenfoods1 = pd.concat([chosenfoods1,rk1])

# --- Step 6: Export to Excel with frozen header/ID ---
output_path = "chosenfoods1.xlsx"
chosenfoods1.to_excel(output_path, engine='openpyxl')

# Freeze row 1 and column A
wb = load_workbook(output_path)
ws = wb.active
ws.freeze_panes = ws['B2']
wb.save(output_path)

from datetime import date
import numpy as np
import pandas as pd

# Define IDs for your foods
carrot_id = '11124'        # Raw carrot (example NDB ID)
green_cabbage_id = '11109' # Green cabbage (raw)
sauerkraut_id = '11439'    # Sauerkraut (from green cabbage)
saurkarrot_id = '99998'    # New food ID for inferred fermented carrot

# Copy carrot nutrient profile
sk = npt.loc[carrot_id].astype(float).copy()

# Build transformation ratios from cabbage â†’ kraut
gc = npt.loc[green_cabbage_id].astype(float)
gs = npt.loc[sauerkraut_id].astype(float)

# Use only nutrients both foods have
transform_nutrients = ['203', '204', '208', '269', '212', '291', '401']  # + add '255' if desired
existing_nutrients = [nutr for nutr in transform_nutrients if nutr in npt.columns]

# Avoid divide-by-zero
gc_safe = gc.replace(0, np.nan)
ratios = gs[existing_nutrients] / gc_safe[existing_nutrients]

# Apply transformation to carrot
for nutr in existing_nutrients:
    if pd.notna(ratios[nutr]) and nutr in sk:
        sk[nutr] = round(sk[nutr] * ratios[nutr], 3)

# Override sodium manually (1% salt = 1000 mg per 100g food)
sk['307'] = 1000

# Ensure all index and columns are strings
chosenfoods1.columns = chosenfoods1.columns.astype(str)
sk.index = sk.index.astype(str)

# Build 1-row DataFrame
new_row_df = pd.DataFrame(sk).T
new_row_df.index = [saurkarrot_id]
new_row_df.columns = new_row_df.columns.astype(str)

# Add metadata
new_row_df['Long_Desc'] = 'Fermented carrot (1% salt, inferred)'
new_row_df['price'] = 0.20
new_row_df['date_modified'] = date.today().isoformat()

# Append to chosenfoods1 with new columns if needed
chosenfoods1 = pd.concat([chosenfoods1, new_row_df], axis=0, sort=False)

import pandas as pd
import numpy as np
from openpyxl import load_workbook
from datetime import date
# --- Step 6: Export to Excel with frozen header/ID ---
output_path = "chosenfoods1.xlsx"
chosenfoods1.to_excel(output_path, engine='openpyxl')

# Freeze row 1 and column A
wb = load_workbook(output_path)
ws = wb.active
ws.freeze_panes = ws['B2']
wb.save(output_path)

rk.index.values

chosenfoods1.columns.values

import pandas as pd
import numpy as np
import warnings

# Food IDs
green_cabbage_id = '11109'
green_sauerkraut_id = '11439'
red_cabbage_id = '11112'
red_sauerkraut_id = '99999'

sodium_nutr_no = '307'  # sodium

# Step 1: Extract foods safely
try:
    gc = uspyomo.npt.loc[green_cabbage_id]
    gs = uspyomo.npt.loc[green_sauerkraut_id]
    rc = uspyomo.npt.loc[red_cabbage_id]
except KeyError as e:
    raise ValueError(f"Missing food entry for NDB ID: {e}")

# Step 2: Compute transformation ratio with divide-by-zero safety
gc_safe = gc.replace(0, np.nan).astype(float)
gs_safe = gs.astype(float)
rc_safe = rc.astype(float)

with pd.option_context('mode.use_inf_as_na', True):
    transformation_ratio = gs_safe / gc_safe

# Step 3: Identify and warn about extreme multipliers
extremely_high = transformation_ratio[transformation_ratio > 10]
extremely_low = transformation_ratio[transformation_ratio < 0.1]

if not extremely_high.empty:
    warnings.warn(f"Unusually high transformation ratios:\n{extremely_high}")

if not extremely_low.empty:
    warnings.warn(f"Unusually low transformation ratios:\n{extremely_low}")

# Step 4: Apply transformation
red_sauerkraut = rc_safe * transformation_ratio

# Step 5: Sodium override (only if present and not NaN)
if pd.notna(gs.get(sodium_nutr_no)):
    red_sauerkraut[sodium_nutr_no] = gs[sodium_nutr_no]

# Step 6: Final cleanup
red_sauerkraut = red_sauerkraut.fillna(0).round(3)

# Step 7: Insert into main DataFrame
uspyomo.npt.loc[red_sauerkraut_id] = red_sauerkraut

uspyomo.npt.loc[red_sauerkraut_id]

uspyomo.npt.loc['11012', 'price']#,uspyomo.npt.loc['A0102', '304'],uspyomo.npt.loc['A0100', '255']

uspyomo.chosenfoods1.loc['A0102', 304]#,uspyomo.npt.loc['A0102', '304'],
#uspyomo.chosenfoods1.loc['A0100', '255']

"""# uscplex1"""

r=uspyomo.UScplex1(1200)
fl3=pd.read_excel('multiindex.xlsx',engine='openpyxl',dtype={'NDB_No':str,'Seq':str}).fillna(method='ffill')
fl4=fl3.set_index(['NDB_No','Seq'])#[['step']]
mi = fl4.index#pd.MultiIndex.from_arrays([list('abb'), list('def')])
ml=mi.get_locs((slice(None),'0'))
mi[0][0],mi[1][1]
fl5=fl4.iloc[ml,1]#the step values# ['meas amount']#.values#mi[0]
fl4.iloc[ml,[1,3]]#the measure descripton and step
fl6=fl5.reset_index()[['NDB_No','Gm_Wgt']]#.values.tolist()
fl6#.reset_index().values.tolist()
fl8=fl6[fl6.NDB_No.isin(uspyomo.chosenfoods1.index)]
fl7=fl8.values.tolist()
fl9=fl4.iloc[ml,[1,3]].reset_index().set_index('NDB_No')[['Gm_Wgt','Msre_Desc']]
from datetime import datetime

datetime.now().strftime("%d/%m/%Y %H:%M:%S")

# fl7 is like ['23200', 0.05]

uspyomo.maketrade(['601pw','613'])#list of 2 npt index values, equates lowslack0 and hislack1. cholesterol and c16

#makes sure these are enabled in chosenfoods and vice versa
intvits=['A0000','A0001','A0002','A0003','A0004','A0005','A0006','A0009','A0010','A0015','A0017','A0018','A0025','A0026',"A0029","A0031","A0033","A0034","A0035","A0038","A0039"]#,'A0013','A0012''A0027']#,'A0016','A0019''A0011',,'A0007','A0008','A0004','A0023'] #etc. could also make a column in chosenfoods1, but ..
mfgex=intvits+['04531','04589','42231','14355','14260','A0025','11593','A0028',"02028","A0034","C0000","C0001","C0002","18375"]#not counted in the 14 (because of low storage/prep cost)
#soybean leithin,cod liver oil, flaseed oil,black tea,decaf green tea,bcaa,tmg,raw waxgourd,potas chloride,paprika
#A0026 is selenium, should it be exclude, or be counted as food ? because of the variability of food source and the consequences of both excess and insufficiency and lack of body storage? 2023-10-09
#lecithin caps, cod liver oil caps;flax oil caps;black tea; just pass the lists into UScplex1...

mfg_test=[{"n":1,"list":['A0015','A0017']}]#2023-04-20 generalized maxfromgroup,  not tested.
#also generalize the minfromgroup, for legume diet
#just enter the seq ?(see below how easy that might be)
intvits=list(zip(intvits,[1]*len(intvits)))#+fl7 #2023-03-29 fl7 is the measures quantization#+intfoods 2022-12-26 fl6 from spreadsheet now, todo put those in multiindex and get rid of the list append here

# Commented out IPython magic to ensure Python compatibility.
# %pdb on

#dont ever use mfgex until that gets debugged. it throws off the mapping to chosenfoods_cplex 2025-4-7

r=uspyomo.UScplex1int(mixed=True,binconsl=intvits,mfg=None,mn=26,mfgex=[])#mfgex)#mn=16#this should return the cvxpy.problem to cache i

#for scip, use this version that doesnt' do quantization of amounts
r=uspyomo.UScplex1int(mixed=True,binconsl=None,mfg=None,mn=24,mfgex=mfgex)#mn=16#this should return the cvxpy.problem to cache i


from google.colab import auth
auth.authenticate_user()

import gspread
from google.auth import default
creds, _ = default()

gc = gspread.authorize(creds)

def fineprint(flu):#2024-01-30 stuck this here because its called for both neos and local
#column ordering; subcolumns for nutrient ranges
    nptsol=uspyomo.npt.loc[nzf2.index]
    solm = nptsol.mul(nzf2.amounts, axis=0)#foodlist

    if (True):#normalize for horizontal sort
      food_totals = solm.sum(axis=0)
      solm = solm.div(food_totals, axis=1)  # Normalize by dividing each value by the sum of its respective column (food)

    solm.rename(columns=nutmap,inplace=True)
    cols= ['dollarsper100g','Fatty acids, total saturated','Fatty acids, total monounsaturated','water','Total Soluble Oxalate (mg) per 100g','citrate']#,'Fatty acids, total polyunsaturated'
    #nutfocus is dumb. make one table, merge it to flu and sort in the page. do this :
    lastcols=[col for col in solm.columns if col not in cols]
    r=cols+lastcols
    #solm=solm[cols+lastcols]#does nothing
    #solm['total']=solm.sum(axis=1)
    #oops, consdf and solm are now differently indexed. rename them all later...

    #2024-6-1 next 4 lines error, but not using so cleanup dead
    #npt2=solm/uscvxpy.consdf['max']
    #npt2['p']='max'
    #npt3=solm/uscvxpy.consdf['min']#some zeros, give NAN
    #npt3['p']='min'

    npt5=solm#/uscvxpy.consdf['min']#some zeros, give NAN
    npt5['p']='abs'


    npt4=pd.concat([npt5],axis=0,join='inner',ignore_index=False)##npt4=pd.concat([npt5,npt2,npt3],axis=0,join='inner',ignore_index=False)#,left_index=True,right_index=True)
    #npt4.reset_index(inplace=True)
    #print(npt4.columns.to_list())
    npt4.reset_index(inplace=True,drop=False)
    #print(npt4.columns.to_list())
    npt4.set_index(['NDB_No','p'],inplace=True)#2024-01-31 'NDB_NO' instead if 'index'  ???#2023-05-06 try pivot table instead
    #the first time , this expects 'NDB_No', but after reloading dataset, it wants 'index' ...
    npt4=npt4.unstack(level=1)#reset index first ??

    #flu.columns = pd.MultiIndex.from_arrays([flu.columns,['']*len(flu.columns)])#2024-3-6 dropping multiindex #2023-05-22

    #https://stackoverflow.com/questions/66880107/adding-a-multi-level-column-to-a-single-level-pandas-dataframe?rq=3
    #return(flu[['Long_Desc','M','Msre_Desc','amounts']].merge(npt4[r],left_index=True,right_index=True))
    return(flu[['Long_Desc','M','Msre_Desc','amounts']].merge(npt5[r],left_index=True,right_index=True))#2024-3-6 dropping the multiindex

#import uscvxpy as uscvxpy
#import importlib; importlib.reload(uscvxpy)

import glob
path = vdsols+'*.zip'# absolute path to search all text files inside a specific folder
zf = glob.glob(path)
from zipfile import ZipFile
with ZipFile(zf[0], 'r') as zObject:# loading the temp.zip and creating a zip object
    zObject.extractall(path=vdsols)# Extracting all the members of the zipinto a specific location.
#os.remove(zf[0])#delete the zipfile
nutmap=uspyomo.consdf[uspyomo.consdf.NutrDesc.notnull()].NutrDesc.to_dict()
import shutil
#import xlswriter
sols1=os.listdir(vdsols)
sols1 = [filename for filename in sols1 if filename.startswith('model') and filename.endswith('.sol')]

numeric_suffixes = [int(filename.split('model')[-1].split('.')[0]) for filename in sols1]

# Sort the filenames based on their numeric suffixes
sols = [f"model{suffix}.sol" for suffix in sorted(numeric_suffixes)]


#should delete the sol files after, in case next solve has fewer
#   if i==0:#if error, dont wipe out previous results by opening new file
fluwriter = pd.ExcelWriter(vd+"flu.xlsx", engine="openpyxl")
slackswriter = pd.ExcelWriter(vd+"slacks.xlsx", engine="openpyxl")

sheet = gc.open_by_key('1U1k0B78SS9nlualX9OmjMx8go09wDZpfyXhzHjpq1ks')#gc.open("flu")

existing_sheets = sheet.worksheets()
# Clear existing sheets except the first one
#for ws in existing_sheets[1:]:
#    sheet.del_worksheet(ws)

for ws in existing_sheets:
    if ws.title.isdigit() and int(ws.title) in range(10):
        sheet.del_worksheet(ws)


for i in sols:
  if i.startswith("model"):# and i[5:].isdigit():# skip the .zip file and the .mst file
    print(i)

    name, ext = os.path.splitext(i)
    number=name[-1]
    name=name[:-1]#trim number
    f=name+ext
    shutil.copyfile(vdsols+i,vd+f)
    #shutil.copy(vdsols+i,vd+i)
    #print(i)
    nzf2=uspyomo.UScplex2(True)
    fl=nzf2[nzf2['amounts']>1e-3][['Long_Desc','amounts']]#,'allmeas']]#2024-5-12 wtf is allmeas


    flu=fl.merge(fl9,left_index=True,right_index=True,how='left')
    flu['M']=flu['amounts']/flu['Gm_Wgt']
    #flu.to_excel('flu.xlsx',freeze_panes=(1, 1))
    flu=fineprint(flu)#add in the food drilldowns as %

    flu= flu.loc[:, ~flu.columns.str.endswith(('Hi', 'Lo'))]#2024-3-6

    #[['Long_Desc','M','Msre_Desc','amounts']]
    obj,st=uspyomo.slackscplex()
    col=['-slacks cost','+slacks cost']#,'+slacks']
    cols=['nuta','-slacks cost','+slacks cost','min','max','-slacks','+slacks']

    st=st[~st.index.str.endswith(('Hi', 'Lo'))]#2024-3-6

    st['NutrDesc'].fillna(st.index.to_series(), inplace=True)
    st1=st.set_index('NutrDesc')
    st1=st1[cols].transpose()#2024-6-2
    flu=pd.concat([flu,st1])#['omegaratio']#.columns.values

    flu.sort_values('amounts',inplace=False,ascending=False).to_excel(fluwriter, sheet_name=number,freeze_panes=(2, 2))#.to_excel('flu.xlsx',freeze_panes=(1, 1))

    st[cols].sort_values(col,ascending=False).to_excel(slackswriter,sheet_name=str(obj),freeze_panes=(1, 1))#.head(10)


    flu_with_headers = flu.fillna('').reset_index().transpose().reset_index().transpose()
    worksheet =sheet.add_worksheet(number,flu.shape[0],flu.shape[1])
    flu_no_nan = flu.fillna('')  # Replace NaN values with empty strings
    cell_values = flu_no_nan.values.tolist()
    index = [[header] for header in flu_no_nan.index.tolist()]
    columns = [[header] for header in flu_no_nan.columns.tolist()]
    data_values = flu_no_nan.values.tolist()
    # Concatenate index, column headers, and data values
    combined_values = index + columns + data_values
    # Update the cells
    worksheet.freeze(rows=1, cols=2)
    worksheet.update('A1', flu_with_headers.values.tolist())#combined_values)
    # Construct the cell range string
    #cell_range = gspread.utils.rowcol_to_a1(3, 3)+":"+gspread.utils.rowcol_to_a1(flu.shape[0]+2, flu.shape[1]+2)#f'{first_column_letter}1:{last_column_letter}{flu.shape[0]}'
    #cell_range = gspread.utils.rowcol_to_a1(1, 1)+":"+gspread.utils.rowcol_to_a1(flu_with_headers.shape[0], flu_with_headers.shape[1])#f'{first_column_letter}1:{last_column_letter}{flu.shape[0]}'
    #worksheet.update(cell_range, flu_with_headers.values.tolist())




    #next 4 lines by chatgpt 2023-12-15
    worksheet = fluwriter.sheets[number]
    worksheet.column_dimensions['B'].width = 50#first_column_width

    if (True):#transpose and percent of total. this goes in fineprint, dont need to deal with text
      flu_numeric = flu.apply(pd.to_numeric, errors='coerce')
      food_totals = flu_numeric.sum(axis=0)  # Calculate the sum of each food (column)
      normalized_df = flu_numeric.div(food_totals, axis=1)  # Normalize by dividing each value by the sum of its respective column (food)

      # Create a mask indicating where numeric values exist in flu
      mask = flu.applymap(lambda x: isinstance(x, (int, float)))

      # Replace numeric values with normalized values, retain text values
      masked_df = flu.mask(mask, normalized_df)

      # Write the masked DataFrame to Excel
      masked_df.transpose().to_excel(fluwriter, sheet_name=number+"T", freeze_panes=(2, 2))

fluwriter.close();slackswriter.close()

expr = sum(round(value(uspyomo.prob.bv[i])) * (1 - uspyomo.prob.bv[i]) + (1 - round(value(uspyomo.prob.bv[i]))) * uspyomo.prob.bv[i] for i in uspyomo.prob.bv)
uspyomo.prob.constraints.add(expr>=1)

# After solving the model:
old_val = {i: value(uspyomo.prob.bv[i]) for i in uspyomo.prob.bv}

# Build the no-good cut expression using the stored values:
expr = sum(old_val[i] * (1 - uspyomo.prob.bv[i]) + (1 - old_val[i]) * uspyomo.prob.bv[i] for i in uspyomo.prob.bv)
uspyomo.prob.constraints.add(expr>=1)

old_val = {i: value(uspyomo.prob.bv[i]) for i in uspyomo.prob.bv}

uspyomo.UScplex2(False)#original problem has 1910 variables (797 bin, 21 int, 0 impl, 1092 cont) and 3045 constraints

for i in uspyomo.prob.bv:
  new_val = round(value(uspyomo.prob.bv[i]))
  old_vali = round(old_val[i])
  if new_val != old_vali:
      print(f"Index {i}: old value = {old_vali}, new value = {new_val}")

changed = []
for i in uspyomo.prob.bv:
    new_val = value(uspyomo.prob.bv[i])
    if new_val != old_val[i]:
        changed.append(i)

old_val[0],value(uspyomo.prob.bv[0])#
uspyomo.chosenfoods_cplex.iloc[changed[0]]

uspyomo.prob.constraints.display()

fluwriter = pd.ExcelWriter(vd+"flu.xlsx", engine="openpyxl")
slackswriter = pd.ExcelWriter(vd+"slacks.xlsx", engine="openpyxl")

sheet = gc.open_by_key('1U1k0B78SS9nlualX9OmjMx8go09wDZpfyXhzHjpq1ks')#gc.open("flu")

existing_sheets = sheet.worksheets()# Clear existing sheets except the first one

for ws in existing_sheets:
    if ws.title.isdigit() and int(ws.title) in range(10):
        sheet.del_worksheet(ws)


for i in range(5):
    nzf2=uspyomo.UScplex2(False)#call local scip to solve
    expr = sum(value(uspyomo.prob.bv[i]) * (1 - uspyomo.prob.bv[i]) + (1 - value(uspyomo.prob.bv[i])) * uspyomo.prob.bv[i] for i in uspyomo.prob.bv)
    uspyomo.prob.constraints.add(expr>=1)




    fl=nzf2[nzf2['amounts']>1e-3][['Long_Desc','amounts']]#,'allmeas']]#2024-5-12 wtf is allmeas


    flu=fl.merge(fl9,left_index=True,right_index=True,how='left')
    flu['M']=flu['amounts']/flu['Gm_Wgt']
    flu=fineprint(flu)#add in the food drilldowns as %

    flu= flu.loc[:, ~flu.columns.str.endswith(('Hi', 'Lo'))]#2024-3-6
    obj,st=uspyomo.slackscplex()
    col=['-slacks cost','+slacks cost']#,'+slacks']
    cols=['nuta','-slacks cost','+slacks cost','min','max','-slacks','+slacks']

    st=st[~st.index.str.endswith(('Hi', 'Lo'))]#2024-3-6

    st['NutrDesc'].fillna(st.index.to_series(), inplace=True)
    st1=st.set_index('NutrDesc')
    st1=st1[cols].transpose()#2024-6-2
    flu=pd.concat([flu,st1])#['omegaratio']#.columns.values

    flu.sort_values('amounts',inplace=False,ascending=False).to_excel(fluwriter, sheet_name=number,freeze_panes=(2, 2))#.to_excel('flu.xlsx',freeze_panes=(1, 1))
    #flu.sort_values('Long_Desc',inplace=False,ascending=False).to_excel(fluwriter, sheet_name=number,freeze_panes=(2, 2))#.to_excel('flu.xlsx',freeze_panes=(1, 1))

    st[cols].sort_values(col,ascending=False).to_excel(slackswriter,sheet_name=str(obj),freeze_panes=(1, 1))#.head(10)


    flu_with_headers = flu.fillna('').reset_index().transpose().reset_index().transpose()
    #worksheet =sheet.add_worksheet(number,flu.shape[0],flu.shape[1])#changed 2025-3-26
    worksheet =sheet.add_worksheet(str(i),flu.shape[0],flu.shape[1])

    flu_no_nan = flu.fillna('')  # Replace NaN values with empty strings
    cell_values = flu_no_nan.values.tolist()
    index = [[header] for header in flu_no_nan.index.tolist()]
    columns = [[header] for header in flu_no_nan.columns.tolist()]
    data_values = flu_no_nan.values.tolist()
    # Concatenate index, column headers, and data values
    combined_values = index + columns + data_values
    # Update the cells
    worksheet.freeze(rows=1, cols=2)
    worksheet.update('A1', flu_with_headers.values.tolist())#combined_values)
 #2025-3-26
 # Now add the note to cell A1 using the Sheets API service:
    service.spreadsheets().batchUpdate(spreadsheetId=sheet.id, body={'requests': [{'updateCells': {'range': {'sheetId': worksheet.id, 'startRowIndex': 0, 'endRowIndex': 1, 'startColumnIndex': 0, 'endColumnIndex': 1}, 'rows': [{'values': [{'note': str(value(uspyomo.prob.obj))}]}], 'fields': 'note'}}]}).execute()


    #next 4 lines by chatgpt 2023-12-15
    worksheet = fluwriter.sheets[number]
    worksheet.column_dimensions['B'].width = 50#first_column_width

    if (True):#transpose and percent of total. this goes in fineprint, dont need to deal with text
      flu_numeric = flu.apply(pd.to_numeric, errors='coerce')
      food_totals = flu_numeric.sum(axis=0)  # Calculate the sum of each food (column)
      normalized_df = flu_numeric.div(food_totals, axis=1)  # Normalize by dividing each value by the sum of its respective column (food)

      # Create a mask indicating where numeric values exist in flu
      mask = flu.applymap(lambda x: isinstance(x, (int, float)))

      # Replace numeric values with normalized values, retain text values
      masked_df = flu.mask(mask, normalized_df)

      # Write the masked DataFrame to Excel
      masked_df.transpose().to_excel(fluwriter, sheet_name=number+"T", freeze_panes=(2, 2))

fluwriter.close();slackswriter.close()

"""## highlighting"""

from googleapiclient.discovery import build
service = build('sheets', 'v4', credentials=creds)

from pyomo.environ import value
nutmap=uspyomo.consdf[uspyomo.consdf.NutrDesc.notnull()].NutrDesc.to_dict()

#2025-9-22 lots of constraints getting piled on, with appearance of redundancy, attracting garbage collection.
# try again with gurobi, figure out the automation. ..

from pyomo.environ import Constraint
def debugme():
    # Use sets for order-independent comparison.
  #import pdb; pdb.set_trace()
  prev_set = set(previous_values)
  current_set = set(current_values)

  # Determine newly added and removed items.
  new_items = sorted(list(current_set - prev_set))
  removed_items = sorted(list(prev_set - current_set))#RWS fix this ? 2025-4-5
  print("New items:", new_items)
  print("Removed items:", removed_items)
  #import pdb; pdb.set_trace()

import pandas as pd
import gspread
#import gspread_formatting
from gspread_formatting import format_cell_range, cellFormat, color
import gspread.utils

# Assume these are already defined:
#   - vd (your directory path)
#   - gc (your authenticated gspread client)
#   - service (your Google Sheets API service, if needed for batchUpdate)
#   - uspyomo, fineprint, fl9, etc.
#
# Create Excel writers if needed:
fluwriter = pd.ExcelWriter(vd + "flu.xlsx", engine="openpyxl")
slackswriter = pd.ExcelWriter(vd + "slacks.xlsx", engine="openpyxl")

# Open the Google Sheets spreadsheet
sheet = gc.open_by_key('1U1k0B78SS9nlualX9OmjMx8go09wDZpfyXhzHjpq1ks')

# Delete any worksheets whose title is a digit (within a certain range)
existing_sheets = sheet.worksheets()
for ws in existing_sheets:
    if ws.title.isdigit():
        sheet.del_worksheet(ws)

# Set up (or create) the Summary worksheet.
summary_headers = ["Iteration", "Objective", "New Items", "Removed Items"]
try:
    summary_ws = sheet.worksheet("Summary")
    summary_ws.clear()
    # Write header row.
    summary_ws.update('A1', [summary_headers])
except gspread.exceptions.WorksheetNotFound:
    summary_ws = sheet.add_worksheet(title="Summary", rows="100", cols="10")
    summary_ws.update('A1', [summary_headers])

# Main loop (change to while True: for indefinite execution if desired)
for i in range(10):
    # -------------------------
    # Data Processing and Sheet Creation
    # -------------------------
    nzf2 = uspyomo.UScplex2(False)  # call local scip to solve
    if i>1:
      print("no-good cut:",value(expr))
    expr = sum(
        round(value(uspyomo.prob.bv[j])) * (1 - uspyomo.prob.bv[j]) +
        (1 - round(value(uspyomo.prob.bv[j]))) * uspyomo.prob.bv[j]
        for j in uspyomo.prob.bv
    )
    uspyomo.prob.constraints.add(expr >= 1)

    fl = nzf2[nzf2['amounts'] > 1e-3][['Long_Desc', 'amounts']]
    flu = fl.merge(fl9, left_index=True, right_index=True, how='left')
    flu['M'] = flu['amounts'] / flu['Gm_Wgt']
    flu = fineprint(flu)  # add in the food drilldowns as %
    flu = flu.loc[:, ~flu.columns.str.endswith(('Hi', 'Lo'))]

    obj, st = uspyomo.slackscplex()
    col = ['-slacks cost', '+slacks cost']
    cols = ['nuta', '-slacks cost', '+slacks cost', 'min', 'max', '-slacks', '+slacks']

    st = st[~st.index.str.endswith(('Hi', 'Lo'))]
    st['NutrDesc'].fillna(st.index.to_series(), inplace=True)
    st1 = st.set_index('NutrDesc')
    st1 = st1[cols].transpose()
    flu = pd.concat([flu, st1])

    # Write flu to Excel files (if needed)
    flu.sort_values('amounts', ascending=False).to_excel(fluwriter, sheet_name=str(i), freeze_panes=(2, 2))
    st[cols].sort_values(col, ascending=False).to_excel(slackswriter, sheet_name=str(obj), freeze_panes=(1, 1))

    # Prepare flu for Google Sheets update (with headers)
    flu_with_headers = flu.fillna('').reset_index().transpose().reset_index().transpose()

    # Create a new worksheet in the Google Sheet (using the iteration number as title)
    worksheet = sheet.add_worksheet(str(i), flu.shape[0], flu.shape[1])

    # Freeze the first row and first two columns
    worksheet.freeze(rows=1, cols=2)

    # Determine the cell range for the update (starting at A1)
    num_rows, num_cols = flu_with_headers.shape
    cell_range = gspread.utils.rowcol_to_a1(1, 1) + ":" + gspread.utils.rowcol_to_a1(num_rows, num_cols)

    # Update the worksheet with the DataFrame values
    worksheet.update(cell_range, flu_with_headers.values.tolist())

    # (Optional) Add a note to cell A1 using the Sheets API service
    service.spreadsheets().batchUpdate(
        spreadsheetId=sheet.id,
        body={
            'requests': [{
                'updateCells': {
                    'range': {
                        'sheetId': worksheet.id,
                        'startRowIndex': 0,
                        'endRowIndex': 1,
                        'startColumnIndex': 0,
                        'endColumnIndex': 1
                    },
                    'rows': [{
                        'values': [{'note': str(value(uspyomo.prob.obj))}]
                    }],
                    'fields': 'note'
                }
            }]
        }
    ).execute()

    # (Optional) Adjust Excel column width for the Excel file, if applicable
    worksheet_ex = fluwriter.sheets[str(i)]
    worksheet_ex.column_dimensions['B'].width = 50

    # -------------------------
    # Bi-Directional Highlighting and In-Loop Summary Update
    # -------------------------
    # Locate the "Long_Desc" column from the header row.
    header = worksheet.row_values(1)
    if "Long_Desc" not in header:
        raise ValueError('Column "Long_Desc" not found in header row.')
    col_index = header.index("Long_Desc") + 1  # Convert to 1-indexed

    # Get current "Long_Desc" values (skipping header)
    current_values = worksheet.col_values(col_index)[1:]

    # Retrieve the objective function value (as a string) for this iteration.
    # (Adjust the extraction as needed; here we assume value(uspyomo.prob.obj) provides the value.)
    obj_value = str(value(uspyomo.prob.obj))

    if i == 0:
        # For the first iteration, there's no previous sheet.
        summary_row = [str(i), obj_value, "\n"]
    else:
        # Get previous worksheet (by title, assumed to be str(i-1))
        prev_ws = sheet.worksheet(str(i - 1))
        previous_values = prev_ws.col_values(col_index)[1:]

        prev_set = set(previous_values)
        current_set = set(current_values)

        # Determine newly added and removed items.
        new_items = sorted(list(current_set - prev_set))
        removed_items = sorted(list(prev_set - current_set))#RWS fix this ? 2025-4-5
        print("New items:", new_items)
        print("Removed items:", removed_items)
        # # Use sets for order-independent comparison.
        # prev_set = set(previous_values)
        # current_set = set(current_values)

        # # Determine newly added and removed items.
        # new_items = sorted(list(current_set - prev_set))
        # removed_items = sorted(list(prev_set - current_set))#RWS fix this ? 2025-4-5

        # Highlight new items in current sheet (yellow)
        for row_num, val in enumerate(current_values, start=2):
            if val not in prev_set:
                cell_a1 = gspread.utils.rowcol_to_a1(row_num, col_index)
                fmt = cellFormat(backgroundColor=color(1, 1, 0))  # Yellow
                #gspread_formatting.format_cell_range(worksheet, cell_a1, fmt)
                format_cell_range(worksheet, cell_a1, fmt)
        # Highlight removed items in previous sheet (cyan)
        for row_num, val in enumerate(previous_values, start=2):
            if val not in current_set:
                cell_a1 = gspread.utils.rowcol_to_a1(row_num, col_index)
                fmt = cellFormat(backgroundColor=color(0, 1, 1))  # Cyan
                format_cell_range(prev_ws, cell_a1, fmt)

        summary_row = [str(i), obj_value, "\n".join(new_items), "\n ".join(removed_items)]

    # Append the summary row directly to the "Summary" sheet.
    summary_ws.append_row(summary_row, value_input_option='USER_ENTERED')
    from gspread_formatting import format_cell_range, cellFormat
    wrap_fmt = cellFormat(wrapStrategy="WRAP")
    format_cell_range(summary_ws, "C:D", wrap_fmt)
    # Optional: Print log information
    print(f"Iteration {i} complete. Summary: {summary_row}")

# End of loop

fluwriter.close()
slackswriter.close()



format_cell_range()

def debugme():
    # Use sets for order-independent comparison.
  #import pdb; pdb.set_trace()
  prev_set = set(previous_values)
  current_set = set(current_values)

  # Determine newly added and removed items.
  new_items = sorted(list(current_set - prev_set))
  removed_items = sorted(list(prev_set - current_set))#RWS fix this ? 2025-4-5
  print("New items:", new_items)
  print("Removed items:", removed_items)
  #import pdb; pdb.set_trace()

import pandas as pd
import gspread
import gspread_formatting
from gspread_formatting import format_cell_range, cellFormat, color
import gspread.utils

# Assume these are already defined:
#   - vd (your directory path)
#   - gc (your authenticated gspread client)
#   - service (your Google Sheets API service, if needed for batchUpdate)
#   - uspyomo, fineprint, fl9, etc.
#
# Create Excel writers if needed:
fluwriter = pd.ExcelWriter(vd + "flu.xlsx", engine="openpyxl")
slackswriter = pd.ExcelWriter(vd + "slacks.xlsx", engine="openpyxl")

# Open the Google Sheets spreadsheet
sheet = gc.open_by_key('1U1k0B78SS9nlualX9OmjMx8go09wDZpfyXhzHjpq1ks')

# Delete any worksheets whose title is a digit (within a certain range)
existing_sheets = sheet.worksheets()
for ws in existing_sheets:
    if ws.title.isdigit():
        sheet.del_worksheet(ws)

# Set up (or create) the Summary worksheet.
summary_headers = ["Iteration", "Objective", "New Items", "Removed Items"]
try:
    summary_ws = sheet.worksheet("Summary")
    summary_ws.clear()
    # Write header row.
    summary_ws.update('A1', [summary_headers])
except gspread.exceptions.WorksheetNotFound:
    summary_ws = sheet.add_worksheet(title="Summary", rows="100", cols="10")
    summary_ws.update('A1', [summary_headers])

diet_history = []  # This will store actual 0/1 values, not variable references

# Main loop (change to while True: for indefinite execution if desired)
for i in range(150):


# BEFORE SOLVING: Add constraints to exclude all previous diets
    if i > 0:
        for diet_num, previous_diet in enumerate(diet_history):
            # This is the key: we're using STORED VALUES from previous_diet
            # combined with VARIABLE REFERENCES from prob.bv

            # Create the no-good cut:
            # "At least one food must be different from this previous diet"

            # In C terms, this would be like:
            # int hamming_distance = 0;
            # for(j in all_foods) {
            #     if(previous_diet[j] == 1 && current_bv[j] == 0) hamming_distance++;
            #     if(previous_diet[j] == 0 && current_bv[j] == 1) hamming_distance++;
            # }
            # assert(hamming_distance >= 1);

            expr = sum(
                previous_diet[food_id] * (1 - prob.bv[food_id]) +  # Was ON, now OFF
                (1 - previous_diet[food_id]) * prob.bv[food_id]    # Was OFF, now ON
                for food_id in prob.bv
            )

            # Add this as a named constraint (important for debugging)
            constraint_name = f"no_repeat_diet_{diet_num}"
            if hasattr(prob, constraint_name):
                delattr(prob, constraint_name)  # Remove if exists

            setattr(prob, constraint_name, Constraint(expr=expr >= 1))

        print(f"\nIteration {i}: Excluding {len(diet_history)} previous diets")

    # NOW SOLVE with all the no-good cuts in place
    nzf2 = UScplex2(False)

    # AFTER SOLVING: Store this solution's VALUES (not variable references!)
    # This is like memcpy() in C - we're copying the actual values
    current_diet = {}
    selected_foods = []

    for food_id in prob.bv:
        # Dereference and store the actual value (0 or 1)
        food_is_selected = round(value(prob.bv[food_id]))
        current_diet[food_id] = food_is_selected

        if food_is_selected == 1:
            selected_foods.append(food_id)

    # Check for duplicates (this shouldn't happen if cuts are working)
    is_duplicate = False
    for prev_diet in diet_history:
        if all(current_diet[f] == prev_diet[f] for f in prob.bv):
            is_duplicate = True
            print(f"âš ï¸ WARNING: Diet {i} is a duplicate! The no-good cuts may not be working.")
            break

    if not is_duplicate:
        print(f"âœ“ Diet {i}: Found new combination with {len(selected_foods)} foods")
        # Show first few foods for verification
        print(f"  Sample foods: {selected_foods[:5]}...")

    # Add to history (this is our "database" of previous diets)
    diet_history.append(current_diet)



    fl = nzf2[nzf2['amounts'] > 1e-3][['Long_Desc', 'amounts']]
    flu = fl.merge(fl9, left_index=True, right_index=True, how='left')
    flu['M'] = flu['amounts'] / flu['Gm_Wgt']
    flu = fineprint(flu)  # add in the food drilldowns as %
    flu = flu.loc[:, ~flu.columns.str.endswith(('Hi', 'Lo'))]

    obj, st = slackscplex()
    col = ['-slacks cost', '+slacks cost']
    cols = ['nuta', '-slacks cost', '+slacks cost', 'min', 'max', '-slacks', '+slacks']

    st = st[~st.index.str.endswith(('Hi', 'Lo'))]
    st['NutrDesc'].fillna(st.index.to_series(), inplace=True)
    st1 = st.set_index('NutrDesc')
    st1 = st1[cols].transpose()
    flu = pd.concat([flu, st1])

    # Write flu to Excel files (if needed)
    flu.sort_values('amounts', ascending=False).to_excel(fluwriter, sheet_name=str(i), freeze_panes=(2, 2))
    st[cols].sort_values(col, ascending=False).to_excel(slackswriter, sheet_name=str(obj), freeze_panes=(1, 1))

    # Prepare flu for Google Sheets update (with headers)
    flu_with_headers = flu.fillna('').reset_index().transpose().reset_index().transpose()

    # Create a new worksheet in the Google Sheet (using the iteration number as title)
    worksheet = sheet.add_worksheet(str(i), flu.shape[0], flu.shape[1])

    # Freeze the first row and first two columns
    worksheet.freeze(rows=1, cols=2)

    # Determine the cell range for the update (starting at A1)
    num_rows, num_cols = flu_with_headers.shape
    cell_range = gspread.utils.rowcol_to_a1(1, 1) + ":" + gspread.utils.rowcol_to_a1(num_rows, num_cols)

    # Update the worksheet with the DataFrame values
    worksheet.update(cell_range, flu_with_headers.values.tolist())

    # (Optional) Add a note to cell A1 using the Sheets API service
    service.spreadsheets().batchUpdate(
        spreadsheetId=sheet.id,
        body={
            'requests': [{
                'updateCells': {
                    'range': {
                        'sheetId': worksheet.id,
                        'startRowIndex': 0,
                        'endRowIndex': 1,
                        'startColumnIndex': 0,
                        'endColumnIndex': 1
                    },
                    'rows': [{
                        'values': [{'note': str(value(uspyomo.prob.obj))}]
                    }],
                    'fields': 'note'
                }
            }]
        }
    ).execute()

    # (Optional) Adjust Excel column width for the Excel file, if applicable
    worksheet_ex = fluwriter.sheets[str(i)]
    worksheet_ex.column_dimensions['B'].width = 50

    # -------------------------
    # Bi-Directional Highlighting and In-Loop Summary Update
    # -------------------------
    # Locate the "Long_Desc" column from the header row.
    header = worksheet.row_values(1)
    if "Long_Desc" not in header:
        raise ValueError('Column "Long_Desc" not found in header row.')
    col_index = header.index("Long_Desc") + 1  # Convert to 1-indexed

    # Get current "Long_Desc" values (skipping header)
    current_values = worksheet.col_values(col_index)[1:]

    # Retrieve the objective function value (as a string) for this iteration.
    # (Adjust the extraction as needed; here we assume value(uspyomo.prob.obj) provides the value.)
    obj_value = str(value(uspyomo.prob.obj))

    if i == 0:
        # For the first iteration, there's no previous sheet.
        summary_row = [str(i), obj_value, "\n"]
    else:
        # Get previous worksheet (by title, assumed to be str(i-1))
        prev_ws = sheet.worksheet(str(i - 1))
        previous_values = prev_ws.col_values(col_index)[1:]

        prev_set = set(previous_values)
        current_set = set(current_values)

        # Determine newly added and removed items.
        new_items = sorted(list(current_set - prev_set))
        removed_items = sorted(list(prev_set - current_set))#RWS fix this ? 2025-4-5
        print("New items:", new_items)
        print("Removed items:", removed_items)
        # # Use sets for order-independent comparison.
        # prev_set = set(previous_values)
        # current_set = set(current_values)

        # # Determine newly added and removed items.
        # new_items = sorted(list(current_set - prev_set))
        # removed_items = sorted(list(prev_set - current_set))#RWS fix this ? 2025-4-5

        # Highlight new items in current sheet (yellow)
        for row_num, val in enumerate(current_values, start=2):
            if val not in prev_set:
                cell_a1 = gspread.utils.rowcol_to_a1(row_num, col_index)
                fmt = cellFormat(backgroundColor=color(1, 1, 0))  # Yellow
                gspread_formatting.format_cell_range(worksheet, cell_a1, fmt)

        # Highlight removed items in previous sheet (cyan)
        for row_num, val in enumerate(previous_values, start=2):
            if val not in current_set:
                cell_a1 = gspread.utils.rowcol_to_a1(row_num, col_index)
                fmt = cellFormat(backgroundColor=color(0, 1, 1))  # Cyan
                gspread_formatting.format_cell_range(prev_ws, cell_a1, fmt)

        summary_row = [str(i), obj_value, "\n".join(new_items), "\n ".join(removed_items)]

    # Append the summary row directly to the "Summary" sheet.
    summary_ws.append_row(summary_row, value_input_option='USER_ENTERED')
    from gspread_formatting import format_cell_range, cellFormat
    wrap_fmt = cellFormat(wrapStrategy="WRAP")
    format_cell_range(summary_ws, "C:D", wrap_fmt)
    # Optional: Print log information
    print(f"Iteration {i} complete. Summary: {summary_row}")

# End of loop

fluwriter.close()
slackswriter.close()

uspyomo.chosenfoods1.index.values

def debugme():
    # Use sets for order-independent comparison.
  import pdb; pdb.set_trace()
  prev_set = set(previous_values)
  current_set = set(current_values)

  # Determine newly added and removed items.
  new_items = sorted(list(current_set - prev_set))
  removed_items = sorted(list(prev_set - current_set))#RWS fix this ? 2025-4-5
  print("New items:", new_items)
  print("Removed items:", removed_items)
  import pdb; pdb.set_trace()

import pandas as pd
import gspread
import gspread_formatting
from gspread_formatting import format_cell_range, cellFormat, color
import gspread.utils

# Assume these are already defined:
#   - vd (your directory path)
#   - gc (your authenticated gspread client)
#   - service (your Google Sheets API service, used for batchUpdate if needed)
#   - uspyomo, fineprint, fl9, etc.
#
# Create Excel writers (if still needed)
fluwriter = pd.ExcelWriter(vd+"flu.xlsx", engine="openpyxl")
slackswriter = pd.ExcelWriter(vd+"slacks.xlsx", engine="openpyxl")

# Open the Google Sheets spreadsheet
sheet = gc.open_by_key('1U1k0B78SS9nlualX9OmjMx8go09wDZpfyXhzHjpq1ks')

# Clear existing sheets whose titles are digits in the desired range
existing_sheets = sheet.worksheets()
for ws in existing_sheets:
    if ws.title.isdigit() and int(ws.title) in range(10):
        sheet.del_worksheet(ws)

# Loop over 5 iterations (adjust as needed)
for i in range(5):
    nzf2 = uspyomo.UScplex2(False)  # call local scip to solve
    expr = sum(value(uspyomo.prob.bv[i]) * (1 - uspyomo.prob.bv[i]) + (1 - value(uspyomo.prob.bv[i])) * uspyomo.prob.bv[i]
               for i in uspyomo.prob.bv)
    uspyomo.prob.constraints.add(expr >= 1)

    fl = nzf2[nzf2['amounts'] > 1e-3][['Long_Desc', 'amounts']]

    flu = fl.merge(fl9, left_index=True, right_index=True, how='left')
    flu['M'] = flu['amounts'] / flu['Gm_Wgt']
    flu = fineprint(flu)  # add in the food drilldowns as %

    flu = flu.loc[:, ~flu.columns.str.endswith(('Hi', 'Lo'))]
    obj, st = uspyomo.slackscplex()
    col = ['-slacks cost', '+slacks cost']
    cols = ['nuta', '-slacks cost', '+slacks cost', 'min', 'max', '-slacks', '+slacks']

    st = st[~st.index.str.endswith(('Hi', 'Lo'))]
    st['NutrDesc'].fillna(st.index.to_series(), inplace=True)
    st1 = st.set_index('NutrDesc')
    st1 = st1[cols].transpose()
    flu = pd.concat([flu, st1])

    # Write flu to Excel (using your preferred sorting)
    flu.sort_values('amounts', ascending=False).to_excel(fluwriter, sheet_name=str(i), freeze_panes=(2, 2))
    st[cols].sort_values(col, ascending=False).to_excel(slackswriter, sheet_name=str(obj), freeze_panes=(1, 1))

    # Prepare flu for Google Sheets update (with headers)
    flu_with_headers = flu.fillna('').reset_index().transpose().reset_index().transpose()

    # Create a new worksheet in the Google Sheet with name = str(i)
    worksheet = sheet.add_worksheet(str(i), flu.shape[0], flu.shape[1])

    # Freeze first row and first two columns
    worksheet.freeze(rows=1, cols=2)

    # Update cells with the DataFrame values (starting at A1)
    worksheet.update('A1', flu_with_headers.values.tolist())

    # (Optional) Add a note to cell A1 â€“ your existing code using service.spreadsheets().batchUpdate
    service.spreadsheets().batchUpdate(
        spreadsheetId=sheet.id,
        body={
            'requests': [{
                'updateCells': {
                    'range': {
                        'sheetId': worksheet.id,
                        'startRowIndex': 0,
                        'endRowIndex': 1,
                        'startColumnIndex': 0,
                        'endColumnIndex': 1
                    },
                    'rows': [{
                        'values': [{'note': str(value(uspyomo.prob.obj))}]
                    }],
                    'fields': 'note'
                }
            }]
        }
    ).execute()

    # (Optional) Adjust column width in the Excel sheet you wrote earlier
    worksheet_ex = fluwriter.sheets[str(i)]
    worksheet_ex.column_dimensions['B'].width = 50

    # --- Integrated Highlighting of "Long_Desc" ---
    # If not the first sheet, compare with the previous worksheet to highlight changes
    if i > 0:
        # Get previous worksheet by its title (assumed to be the previous iteration's number)
        prev_ws = sheet.worksheet(str(i - 1))

        # Retrieve header from current sheet to locate "Long_Desc"
        header = worksheet.row_values(1)
        if "Long_Desc" in header:
            col_index = header.index("Long_Desc") + 1  # 1-indexed

            # Get the current "Long_Desc" column (excluding header)
            current_long_desc = worksheet.col_values(col_index)[1:]
            # Get the previous "Long_Desc" column (excluding header)
            previous_long_desc = prev_ws.col_values(col_index)[1:]

            # Use a set for the previous values (order doesn't matter)
            prev_set = set(previous_long_desc)

            # For each cell in the current sheet's "Long_Desc" column, if its value is not in prev_set, highlight it.
            for row_num, val in enumerate(current_long_desc, start=2):
                if val not in prev_set:
                    cell_a1 = gspread.utils.rowcol_to_a1(row_num, col_index)
                    fmt = cellFormat(backgroundColor=color(1, 1, 0))  # Yellow background
                    gspread_formatting.format_cell_range(worksheet, cell_a1, fmt)

    # End of iteration loop

fluwriter.close()
slackswriter.close()

import pandas as pd
import gspread
import gspread_formatting
from gspread_formatting import format_cell_range, cellFormat, color
import gspread.utils

# --- Your existing setup ---
# Assume these are already defined:
#   - vd (your directory path)
#   - gc (your authenticated gspread client)
#   - service (your Google Sheets API service for batchUpdate if needed)
#   - uspyomo, fineprint, fl9, etc.

fluwriter = pd.ExcelWriter(vd + "flu.xlsx", engine="openpyxl")
slackswriter = pd.ExcelWriter(vd + "slacks.xlsx", engine="openpyxl")

# Open the Google Sheets spreadsheet
sheet = gc.open_by_key('1U1k0B78SS9nlualX9OmjMx8go09wDZpfyXhzHjpq1ks')

# Delete any worksheets whose title is a digit and within a certain range
existing_sheets = sheet.worksheets()
for ws in existing_sheets:
    if ws.title.isdigit() and int(ws.title) in range(10):
        sheet.del_worksheet(ws)

# Loop over 5 iterations (or however many you require)
for i in range(5):
    nzf2 = uspyomo.UScplex2(False)  # call local scip to solve
    expr = sum(value(uspyomo.prob.bv[j]) * (1 - uspyomo.prob.bv[j]) + (1 - value(uspyomo.prob.bv[j])) * uspyomo.prob.bv[j]
               for j in uspyomo.prob.bv)
    uspyomo.prob.constraints.add(expr >= 1)

    fl = nzf2[nzf2['amounts'] > 1e-3][['Long_Desc', 'amounts']]
    flu = fl.merge(fl9, left_index=True, right_index=True, how='left')
    flu['M'] = flu['amounts'] / flu['Gm_Wgt']
    flu = fineprint(flu)  # add in the food drilldowns as %
    flu = flu.loc[:, ~flu.columns.str.endswith(('Hi', 'Lo'))]

    obj, st = uspyomo.slackscplex()
    col = ['-slacks cost', '+slacks cost']
    cols = ['nuta', '-slacks cost', '+slacks cost', 'min', 'max', '-slacks', '+slacks']

    st = st[~st.index.str.endswith(('Hi', 'Lo'))]
    st['NutrDesc'].fillna(st.index.to_series(), inplace=True)
    st1 = st.set_index('NutrDesc')
    st1 = st1[cols].transpose()
    flu = pd.concat([flu, st1])

    # Write flu to Excel files (sorting as desired)
    flu.sort_values('amounts', ascending=False).to_excel(fluwriter, sheet_name=str(i), freeze_panes=(2, 2))
    st[cols].sort_values(col, ascending=False).to_excel(slackswriter, sheet_name=str(obj), freeze_panes=(1, 1))

    # Prepare flu for Google Sheets update (with headers)
    flu_with_headers = flu.fillna('').reset_index().transpose().reset_index().transpose()

    # Create a new worksheet in the Google Sheet (using the iteration number as title)
    worksheet = sheet.add_worksheet(str(i), flu.shape[0], flu.shape[1])

    # Freeze the first row and first two columns
    worksheet.freeze(rows=1, cols=2)

    # Determine the cell range for the update (starting at A1)
    num_rows, num_cols = flu_with_headers.shape
    cell_range = gspread.utils.rowcol_to_a1(1, 1) + ":" + gspread.utils.rowcol_to_a1(num_rows, num_cols)

    # Update the worksheet with the DataFrame values
    worksheet.update(cell_range, flu_with_headers.values.tolist())

    # (Optional) Add a note to cell A1 via the Sheets API service
    service.spreadsheets().batchUpdate(
        spreadsheetId=sheet.id,
        body={
            'requests': [{
                'updateCells': {
                    'range': {
                        'sheetId': worksheet.id,
                        'startRowIndex': 0,
                        'endRowIndex': 1,
                        'startColumnIndex': 0,
                        'endColumnIndex': 1
                    },
                    'rows': [{
                        'values': [{'note': str(value(uspyomo.prob.obj))}]
                    }],
                    'fields': 'note'
                }
            }]
        }
    ).execute()

    # (Optional) Adjust Excel column width for sheet (if applicable)
    worksheet_ex = fluwriter.sheets[str(i)]
    worksheet_ex.column_dimensions['B'].width = 50

    # --- Bi-Directional Highlighting of "Long_Desc" ---
    # We assume the header row (row 1) contains "Long_Desc"
    header = worksheet.row_values(1)
    if "Long_Desc" not in header:
        raise ValueError('Column "Long_Desc" not found in header row.')
    col_index = header.index("Long_Desc") + 1  # Convert to 1-indexed column number

    # For forward (newly added) items: compare current vs. previous
    if i > 0:
        prev_ws = sheet.worksheet(str(i - 1))
        # Get "Long_Desc" values (skip header) from both sheets
        current_values = worksheet.col_values(col_index)[1:]
        previous_values = prev_ws.col_values(col_index)[1:]

        # For order-independent comparison, use sets
        prev_set = set(previous_values)
        current_set = set(current_values)

        # Newly added items (in current but not in previous): highlight in yellow
        for row_num, val in enumerate(current_values, start=2):
            if val not in prev_set:
                cell_a1 = gspread.utils.rowcol_to_a1(row_num, col_index)
                fmt = cellFormat(backgroundColor=color(1, 1, 0))  # Yellow
                gspread_formatting.format_cell_range(worksheet, cell_a1, fmt)

        # Removed items (in previous but not in current): highlight in cyan in the previous sheet
        for row_num, val in enumerate(previous_values, start=2):
            if val not in current_set:
                cell_a1 = gspread.utils.rowcol_to_a1(row_num, col_index)
                fmt = cellFormat(backgroundColor=color(0, 1, 1))  # Cyan
                gspread_formatting.format_cell_range(prev_ws, cell_a1, fmt)

# Close Excel writers
fluwriter.close()
slackswriter.close()

import gspread
import gspread.utils
from gspread_formatting import format_cell_range, cellFormat, color

# --- Existing code that creates/updates your sheets ---
# Open the Google Sheets spreadsheet
sheet = gc.open_by_key('1U1k0B78SS9nlualX9OmjMx8go09wDZpfyXhzHjpq1ks')

# Clear existing sheets except the first one
existing_sheets = sheet.worksheets()
for ws in existing_sheets[1:]:
    sheet.del_worksheet(ws)
sheet.add_worksheet(title="NewData", rows="1000", cols="26")

# Get the first sheet and clear its content
worksheet = existing_sheets[0]
worksheet.clear()

# Prepare your DataFrame 'flu' (with a column "Long_Desc") as before
flu_with_headers = flu.fillna('').reset_index().transpose().reset_index().transpose()

# Determine the cell range to update based on the DataFrame shape
cell_range = gspread.utils.rowcol_to_a1(1, 1) + ":" + gspread.utils.rowcol_to_a1(flu_with_headers.shape[0], flu_with_headers.shape[1])

# Update the cells with your DataFrame values
worksheet.update(cell_range, flu_with_headers.values.tolist())

# --- Highlighting changes in the "Long_Desc" column, ignoring order ---

# Retrieve all sheets in the workbook. We assume that the last two represent the current and previous versions.
all_sheets = sheet.worksheets()
if len(all_sheets) >= 2:
    # For forward comparison, compare new sheet to previous sheet.
    new_sheet = all_sheets[-1]
    prev_sheet = all_sheets[-2]

    # Get the header row from the new sheet to locate "Long_Desc"
    header = new_sheet.row_values(1)
    if "Long_Desc" in header:
        col_index = header.index("Long_Desc") + 1  # Convert to 1-indexed

        # Extract the column values (excluding the header) as sets
        prev_values = set(prev_sheet.col_values(col_index)[1:])  # previous sheet
        new_values = new_sheet.col_values(col_index)[1:]           # new sheet

        # For each cell in the new sheet's "Long_Desc" column, highlight if its value isn't in the previous set
        for row_number, cell_value in enumerate(new_values, start=2):  # row 1 is header
            if cell_value not in prev_values:
                # Construct A1 notation for the cell
                cell_a1 = gspread.utils.rowcol_to_a1(row_number, col_index)
                # Apply yellow background to that cell
                fmt = cellFormat(backgroundColor=color(1, 1, 0))  # Yellow
                format_cell_range(new_sheet, cell_a1, fmt)

    # (Optional) For backward comparison (highlight items in previous sheet that no longer appear in new sheet):
    if "Long_Desc" in prev_sheet.row_values(1):
        col_index_prev = prev_sheet.row_values(1).index("Long_Desc") + 1
        new_values_set = set(new_sheet.col_values(col_index_prev)[1:])
        prev_values_list = prev_sheet.col_values(col_index_prev)[1:]
        for row_number, cell_value in enumerate(prev_values_list, start=2):
            if cell_value not in new_values_set:
                cell_a1 = gspread.utils.rowcol_to_a1(row_number, col_index_prev)
                fmt = cellFormat(backgroundColor=color(0, 1, 1))  # Cyan
                format_cell_range(prev_sheet, cell_a1, fmt)



obj,st=uspyomo.slackscplex()

#todo: how to export my google sheets output to xls to do this highlighting ???
import pandas as pd
from openpyxl import load_workbook
from openpyxl.styles import PatternFill

# Load the existing Excel workbook
wb = load_workbook('flu.xlsx')



# Get the sheet names and sort them
#sheet_names = sorted(wb.sheetnames, key=lambda x: int(x))
sheet_names = sorted([sheet for sheet in wb.sheetnames if sheet.isdigit()], key=lambda x: int(x))

# Forward comparison: Compare each sheet with the next one
for i in range(len(sheet_names) - 1):
    current_sheet = wb[sheet_names[i]]
    next_sheet = wb[sheet_names[i + 1]]

    # Get sets of row values for current and next sheets
    current_values = set(current_sheet.cell(row=row, column=1).value for row in range(2, current_sheet.max_row + 1))
    next_values = set(next_sheet.cell(row=row, column=1).value for row in range(2, next_sheet.max_row + 1))

    # Highlight differences in yellow
    for value in next_values - current_values:
        for row in range(2, next_sheet.max_row + 1):
            if next_sheet.cell(row=row, column=1).value == value:
                for col in range(1, next_sheet.max_column + 1):
                    next_sheet.cell(row=row, column=col).fill = PatternFill(start_color="FFFF00", end_color="FFFF00", fill_type="solid")

# Backward comparison: Compare each sheet with the previous one
for i in range(len(sheet_names) - 1, 0, -1):
    current_sheet = wb[sheet_names[i]]
    previous_sheet = wb[sheet_names[i - 1]]

    # Get sets of row values for current and previous sheets
    current_values = set(current_sheet.cell(row=row, column=1).value for row in range(2, current_sheet.max_row + 1))
    previous_values = set(previous_sheet.cell(row=row, column=1).value for row in range(2, previous_sheet.max_row + 1))

    # Highlight differences in cyan
    for value in previous_values - current_values:
        for row in range(2, previous_sheet.max_row + 1):
            if previous_sheet.cell(row=row, column=1).value == value:
                for col in range(1, previous_sheet.max_column + 1):
                    previous_sheet.cell(row=row, column=col).fill = PatternFill(start_color="00FFFF", end_color="00FFFF", fill_type="solid")

# Save the modified workbook
wb.save('your_existing_workbook_with_highlighting.xlsx')

obj

value(uspyomo.prob.obj)

value(uspyomo.prob.bv[0])

uspyomo.npt[uspyomo.npt['515']>0.1]

uspyomo.

import pandas as pd

# Manually creating the DataFrame based on the provided text data.
data = {
    "Foodstuff": [
        "Barley", "Buckwheat flour", "Flour (bread flour)", "Flour (cake flour)", "Flour (pastry flour)",
        "Rice (polished)", "Rice (unpolished)", "Rice (with the bud)", "Almond", "Azuki bean (dried)",
        "Broad bean", "Green-peas (canned)", "Peanut", "Bean curd lees (Okara)", "Deep-fried tofu",
        "Fermented soybean (Natto)", "Freeze-dried tofu", "Green soybean", "Soymilk", "Soybean (dried)",
        "Tofu (Kinu)", "Tofu (Momen, chilled)", "Tofu (Momen, 3 min boiled)", "Cladosiphon okamuranus (Mozuku)",
        "Hijiki", "Kombu", "Nori", "Wakame"
    ],
    "Part": [
        "Cereals", "Cereals", "Cereals", "Cereals", "Cereals", "Cereals", "Cereals", "Cereals",
        "Beans", "Beans", "Beans", "Beans", "Beans", "Soybean products", "Soybean products",
        "Soybean products", "Soybean products", "Soybean products", "Soybean products", "Soybean products",
        "Soybean products", "Soybean products", "Soybean products", "Dried seaweeds", "Dried seaweeds",
        "Dried seaweeds", "Dried seaweeds", "Dried seaweeds"
    ],
    "Adenine": [
        21.6, 35.1, 12.2, 8.1, 12.2, 10.8, 16.2, 14.9, 13.6, 33.8, 14.1, 6.8, 18.9, 15.0, 20.3,
        40.5, 120.4, 20.8, 7.7, 74.3, 7.6, 14.0, 13.1, 5.6, 31.1, 18.0, 216.0, 67.7
    ],
    "Guanine": [
        22.7, 40.8, 13.6, 7.6, 13.6, 15.1, 21.2, 19.6, 13.8, 43.8, 19.9, 12.1, 28.6, 24.0, 32.4,
        51.4, 168.3, 27.2, 11.7, 98.2, 11.2, 16.7, 8.8, 9.4, 76.5, 21.8, 299.2, 148.1
    ],
    "Hypoxanthine": [
        0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.3, 0.0, 1.3, 0.0, 0.0, 4.5, 0.5, 6.8, 0.6,
        0.0, 2.5, 0.0, 0.5, 0.4, 0.0, 0.5, 25.1, 4.7, 73.3, 46.6
    ],
    "Xanthine": [
        0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.7, 0.0, 0.2, "ND", 1.6, 5.0, 1.2, 15.2, 3.7,
        0.0, 0.0, 0.0, 0.7, 0.0, 0.0, 0.0, 0.0, 1.9, 3.2, 0.0
    ],
    "Total": [
        44.3, 75.9, 25.8, 15.7, 25.8, 25.9, 37.4, 34.5, 31.4, 77.6, 35.5, 18.8, 49.1, 48.6,
        54.4, 113.9, 293.1, 47.9, 22.0, 172.5, 20.0, 31.1, 21.9, 15.4, 132.8, 46.4, 591.7, 262.4
    ],
    "Calculated_as_uric_acid": [
        52.1, 89.1, 30.3, 18.5, 30.3, 30.3, 43.7, 40.3, 37.0, 90.8, 41.5, 21.9, 57.1, 56.6,
        63.2, 132.8, 342.0, 56.1, 25.8, 201.7, 23.3, 36.5, 26.0, 18.0, 154.9, 54.5, 695.6, 306.5
    ],
    "Classified_group": [
        1, 2, 1, 1, 1, 1, 1, 1, 1, "2*", 1, 1, 1, 1, 2, 3, "4*", 1, 1, "3*", 1, 1, 1, 1, "3*",
        1, "5*", "4*"
    ]
}

# Convert dictionary to DataFrame
purine_df = pd.DataFrame(data)
purine_df.head()
#2024-10-31 japanese article on food purine database

# Adding the next set of data based on the provided input

data_eggs_dairy_mushrooms_fruits = {
    "Foodstuff": [
        "Chicken egg", "Quail egg", "Cheese", "Grated cheese", "Milk", "Yogurt",
        "Bunapii", "Bunashimeji", "Enokidake", "Eringi", "Hatakeshimeji", "Hiratake",
        "Jewâ€™s-ear (dried)", "Maitake", "Nameko", "Nameko (big type)", "Shiitake (dried)",
        "Shiitake (for Broth, dried)", "Shiitake (raw)", "Shiitake No. 115 (raw)", "Shiitake No. 240 (raw)",
        "Shiitake No. 697 (raw)", "Tsukuritake", "Usu-hiratake", "White aragekikurage",
        "White hiratake", "Yamabushitake", "Yanagimatsutake", "Banana", "Strawberry"
    ],
    "Part": [
        "Eggs", "Eggs", "Dairy products", "Dairy products", "Dairy products", "Dairy products",
        "Mushrooms", "Mushrooms", "Mushrooms", "Mushrooms", "Mushrooms", "Mushrooms",
        "Mushrooms", "Mushrooms", "Mushrooms", "Mushrooms", "Mushrooms", "Mushrooms",
        "Mushrooms", "Mushrooms", "Mushrooms", "Mushrooms", "Mushrooms", "Mushrooms",
        "Mushrooms", "Mushrooms", "Mushrooms", "Mushrooms", "Fruits", "Fruits"
    ],
    "Adenine": [
        0.0, 0.0, 2.7, 8.2, 0.0, 1.4, 12.2, 9.4, 29.7, 5.0, 9.1, 74.3, 54.8, 47.4,
        14.9, 3.8, 202.7, 132.5, 8.3, 5.5, 10.6, 9.9, 28.4, 16.8, 2.3, 27.0, 12.1, 6.1, 1.2, 0.5
    ],
    "Guanine": [
        "ND", "ND", 3.0, 4.2, 0.0, 2.1, 12.3, 8.0, 19.6, 5.3, 5.7, 68.0, 93.0, 38.9,
        13.6, 2.9, 167.7, 97.8, 10.0, 7.6, 11.0, 12.5, 21.2, 15.7, 4.1, 32.8, 20.7, 11.9, 1.7, 1.2
    ],
    "Hypoxanthine": [
        0.0, 0.0, 0.0, "ND", 0.0, 1.5, 3.2, 1.1, 0.0, 0.7, 0.7, 0.0, 7.9, 7.9,
        "ND", 1.4, 0.0, 11.5, 1.6, 0.5, 4.5, 6.1, 0.0, 1.9, 0.4, 2.6, 0.7, 4.9, 0.1, 0.5
    ],
    "Xanthine": [
        0.0, "ND", 0.0, 0.6, 0.0, 0.2, 3.0, 2.3, 0.0, 2.4, 0.5, 0.0, 0.0, 4.4,
        0.0, 1.3, 9.1, 0.5, 0.9, 2.1, 0.0, 1.0, 0.0, 3.5, 0.1, 4.3, 0.0, 3.3, 0.0, 0.0
    ],
    "Total": [
        0.0, 0.0, 5.7, 12.9, 0.0, 5.2, 30.8, 20.8, 49.4, 13.4, 16.0, 142.3, 155.7,
        98.5, 28.5, 9.5, 379.5, 242.3, 20.8, 15.6, 26.1, 29.5, 49.5, 37.9, 6.9, 66.7, 33.5,
        26.2, 3.0, 2.1
    ],
    "Calculated_as_uric_acid": [
        0.0, 0.0, 6.7, 15.4, 0.0, 6.2, 36.2, 24.6, 58.8, 15.7, 19.1, 168.1, 181.4,
        116.7, 33.6, 11.2, 448.8, 288.4, 24.4, 18.2, 31.0, 34.9, 58.8, 44.6, 8.0, 78.1, 38.9,
        30.5, 3.5, 2.4
    ],
    "Classified_group": [
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, "3*", 2, 1, 1, "5*", "4*", 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1
    ]
}

# Convert dictionary to DataFrame and concatenate with the previous one
purine_df_next = pd.DataFrame(data_eggs_dairy_mushrooms_fruits)
purine_df_combined = pd.concat([purine_df, purine_df_next], ignore_index=True)

purine_df_combined.head(20)  # Display the first 20 rows for a quick preview

# Adding the next set of data based on the provided input for vegetables

data_vegetables = {
    "Foodstuff": [
        "Asparagus Upper part", "Asparagus Lower part", "Avocado", "Balsam pear (goya)", "Bamboo shoot Upper part",
        "Bamboo shoot Lower part", "Bean sprouts", "Broccoli", "Broccoli sprout", "Cabbage", "Carrot", "Cauliflower",
        "Cherry tomato", "Chinese cabbage", "Corn", "Cucumber", "Eggplant", "Garlic", "Garlic chives (nira)", "Ginger",
        "Green pepper", "Gumbo (okura)", "Japanese ginger (myoga)", "Japanese leek (negi)", "Japanese pumpkin",
        "Komatsuna Leaf", "Komatsuna Young leaf", "Onion", "Parsley", "Perilla leaves (shiso)", "Potato", "Spinach Leaf",
        "Spinach Young leaf", "Sprouts (with bean)", "Sweet potato", "White radish sprouts", "Zucchini"
    ],
    "Part": ["Vegetables"] * 37,
    "Adenine": [
        20.7, 3.6, 10.6, 3.5, 24.8, 12.8, 14.1, 25.1, 59.5, 1.3, 0.7, 27.0, 1.6, 2.6, 4.7, 4.2, 11.0,
        6.0, 9.4, 0.4, 15.9, 17.2, 3.1, 12.0, 23.2, 4.2, 13.0, 1.0, 121.5, 19.1, 2.1, 29.8, 83.5,
        28.1, 6.7, 33.9, 5.0
    ],
    "Guanine": [
        30.5, 4.8, 7.5, 4.3, 35.2, 17.0, 14.2, 33.9, 57.2, 1.7, 1.4, 30.2, 1.5, 2.9, 6.9, 5.0, 31.6,
        6.9, 8.5, 1.4, 35.5, 21.3, 3.5, 26.8, 29.1, 6.3, 24.7, 1.1, 135.1, 19.1, 4.2, 13.8, 88.3,
        28.2, 7.3, 29.4, 6.3
    ],
    "Hypoxanthine": [
        3.8, 1.7, 0.3, 1.1, 2.8, 0.7, 3.2, 5.7, 8.1, 0.2, 0.0, "ND", 0.0, 1.2, 0.1, 0.1, 6.6, 3.4,
        1.4, 0.0, 7.0, 0.3, 0.7, 2.6, 1.9, 0.0, 0.0, 0.1, 32.3, 3.1, 0.2, 0.0, 0.0, 0.0, 2.4, 6.4, 1.0
    ],
    "Xanthine": [
        0.3, 0.1, 0.0, 1.0, 0.6, 0.4, 3.5, 5.3, 4.8, 0.0, 0.0, 0.0, 0.0, 0.3, 0.1, 0.1, 1.6, 0.7,
        0.1, 0.5, 10.7, 0.7, 0.4, 0.0, 2.5, 0.0, 1.4, 0.0, 0.0, 0.2, 0.0, 7.7, 0.0, 1.1, 0.6, 3.5, 0.8
    ],
    "Total": [
        55.3, 10.2, 18.4, 9.9, 63.3, 30.8, 35.0, 70.0, 129.6, 3.2, 2.2, 57.2, 3.1, 7.0, 11.7, 9.4,
        50.7, 17.0, 19.4, 2.3, 69.2, 39.5, 7.8, 41.4, 56.6, 10.6, 39.0, 2.3, 288.9, 41.4, 6.5, 51.4,
        171.8, 57.3, 17.0, 73.2, 13.1
    ],
    "Calculated_as_uric_acid": [
        64.7, 12.0, 21.8, 11.6, 74.0, 36.1, 41.2, 81.8, 153.0, 3.8, 2.5, 67.2, 3.7, 8.2, 13.7,
        11.1, 58.7, 20.1, 23.0, 2.5, 79.8, 46.3, 9.2, 48.0, 66.3, 12.3, 45.1, 2.7, 341.3, 49.0, 7.5,
        61.0, 202.1, 67.4, 20.1, 86.6, 15.3
    ],
    "Classified_group": [
        2, 1, 1, 1, 2, 1, 1, 2, 3, 1, 1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 1, 4, 1, 1,
        2, 3, 2, 1, 2, 1
    ]
}

# Convert dictionary to DataFrame and concatenate with the previous one
purine_df_vegetables = pd.DataFrame(data_vegetables)
purine_df_combined = pd.concat([purine_df_combined, purine_df_vegetables], ignore_index=True)

purine_df_combined.tail(20)  # Display the last 20 rows to confirm the addition

import pandas as pd

# Main dataset as a Python dictionary to build the DataFrame. Data for each group is added progressively to match the user's inputs.
# Adding an additional column 'Purine Content Category' to accommodate the last "different format" data

# Sample data for brevity; expand based on the user's input in full implementation
data = {
    "Foodstuff": [
        "Chicken liver", "Half-dried sardine", "Milt (striped pigfish)",
        "Monkfish steamed liver", "Several supplements", "Pork liver",
        "Beef liver", "Bonito", "Sardine", "Okiami (Krill)", "Oriental shrimp",
        "Half-dried Jack mackerel", "Half-dried Pacific saury"
    ],
    "Part": [
        "Very large amount", "Very large amount", "Very large amount",
        "Very large amount", "Very large amount", "Large amount",
        "Large amount", "Large amount", "Large amount", "Large amount",
        "Large amount", "Large amount", "Large amount"
    ],
    "Adenine": [None] * 13,  # Placeholder values as only categorical data is provided in this portion
    "Guanine": [None] * 13,
    "Hypoxanthine": [None] * 13,
    "Xanthine": [None] * 13,
    "Total": [None] * 13,
    "Calculated_as_uric_acid": [None] * 13,
    "Classified_group": [None] * 13,
    "Purine Content Category": [
        "Very large amount (more than 300 mg/100 g)", "Very large amount (more than 300 mg/100 g)",
        "Very large amount (more than 300 mg/100 g)", "Very large amount (more than 300 mg/100 g)",
        "Very large amount (more than 300 mg/100 g)", "Large amount (200â€“300 mg/100 g)",
        "Large amount (200â€“300 mg/100 g)", "Large amount (200â€“300 mg/100 g)",
        "Large amount (200â€“300 mg/100 g)", "Large amount (200â€“300 mg/100 g)",
        "Large amount (200â€“300 mg/100 g)", "Large amount (200â€“300 mg/100 g)",
        "Large amount (200â€“300 mg/100 g)"
    ]
}

# Convert dictionary to DataFrame for the last section
additional_data_df = pd.DataFrame(data)

# Assuming 'purine_df_combined' is the previously assembled DataFrame from the other entries.
# Here we concatenate it with the new category-based data.
final_purine_df = pd.concat([purine_df_combined, additional_data_df], ignore_index=True)

# Displaying the final DataFrame head to verify the combined result
final_purine_df.head(), final_purine_df.tail()

final_purine_df

19/8.28

56/22.7

uspyomo.prob.lsvars[uspyomo.sv1].value

# Assuming uspyomo.lsvars is a Pyomo Var object
# Assuming uspyomo.lsvars.amounts.values returns a 1D NumPy array of values
# Assuming the order of values in the array matches the order of the variables in uspyomo.lsvars

# Get the indices of the variables
var_indices = list(uspyomo.lsvars.keys())

# Create a dictionary mapping indices to values
new_values_dict = dict(zip(var_indices, uspyomo.lsvars.amounts.values))

# Set the values using the dictionary
uspyomo.prob.lsvars.set_values(new_values_dict)

uspyomo.lsvars.amounts.values

def fineprint(flu):#2024-01-30 stuck this here because its called for both neos and local
    nptsol=uspyomo.npt.loc[nzf2.index]
    solm = nptsol.mul(nzf2.amounts, axis=0)#foodlist
    solm.rename(columns=nutmap,inplace=True)
    cols= ['dollarsper100g','Fatty acids, total saturated','Fatty acids, total monounsaturated','water','Total Soluble Oxalate (mg) per 100g','citrate']#,'Fatty acids, total polyunsaturated'
    #nutfocus is dumb. make one table, merge it to flu and sort in the page. do this :
    lastcols=[col for col in solm.columns if col not in cols]
    r=cols+lastcols
    #solm=solm[cols+lastcols]#does nothing
    #solm['total']=solm.sum(axis=1)
    #oops, consdf and solm are now differently indexed. rename them all later...
    npt2=solm/uspyomo.consdf['max']
    npt2['p']='max'
    npt3=solm/uspyomo.consdf['min']#some zeros, give NAN
    npt3['p']='min'

    npt5=solm#/uscvxpy.consdf['min']#some zeros, give NAN
    npt5['p']='abs'

    #npt4=pd.concat([npt5,npt2,npt3],axis=0,join='inner',ignore_index=False)#,left_index=True,right_index=True)
    npt4=pd.concat([npt5],axis=0,join='inner',ignore_index=False)#,left_index=True,right_index=True)

    #npt4.reset_index(inplace=True)
    print(npt4.columns.to_list())
    npt4.reset_index(inplace=True,drop=False)
    print(npt4.columns.to_list())

    npt4.set_index(['index','p'],inplace=True)#2024-01-31 'NDB_NO' instead if 'index'  ???#2023-05-06 try pivot table instead
    #the first time , this expects 'NDB_No', but after reloading dataset, it wants 'index' ...
    npt4=npt4.unstack(level=1)#reset index first ??
    flu.columns = pd.MultiIndex.from_arrays([flu.columns,['']*len(flu.columns)])#2023-05-22
    #https://stackoverflow.com/questions/66880107/adding-a-multi-level-column-to-a-single-level-pandas-dataframe?rq=3
    return(flu[['Long_Desc','M','Msre_Desc','amounts']].merge(npt4[r],left_index=True,right_index=True))

#import uspyomo as uscvxpy
#import importlib; importlib.reload(uscvxpy)

import glob
path = vdsols+'*.zip'# absolute path to search all text files inside a specific folder
zf = glob.glob(path)
from zipfile import ZipFile
with ZipFile(zf[0], 'r') as zObject:# loading the temp.zip and creating a zip object
    zObject.extractall(path=vdsols)# Extracting all the members of the zipinto a specific location.
nutmap=uspyomo.consdf[uspyomo.consdf.NutrDesc.notnull()].NutrDesc.to_dict()
import shutil
#import xlswriter
sols=os.listdir(vdsols)
#should delete the sol files after, in case next solve has fewer
#   if i==0:#if error, dont wipe out previous results by opening new file
fluwriter = pd.ExcelWriter(vd+"flu.xlsx", engine="openpyxl")
slackswriter = pd.ExcelWriter(vd+"slacks.xlsx", engine="openpyxl")
uspyomo.vd=vd#2024-8-27
for i in sols:
    name, ext = os.path.splitext(i)
    number=name[-1]
    name=name[:-1]#trim number
    f=name+ext
    shutil.copyfile(vdsols+i,vd+f)
    #shutil.copy(vdsols+i,vd+i)
    print(i)
    nzf2=uspyomo.UScplex2(True)
    fl=nzf2[nzf2['amounts']>1e-3][['Long_Desc','amounts','allmeas']]

    flu=fl.merge(fl9,left_index=True,right_index=True,how='left')
    flu['M']=flu['amounts']/flu['Gm_Wgt']
    #flu.to_excel('flu.xlsx',freeze_panes=(1, 1))
    flu=fineprint(flu)#add in the food drilldowns as %
    #[['Long_Desc','M','Msre_Desc','amounts']]
    obj,st=uspyomo.slackscplex()
    col=['-slacks cost','+slacks cost']#,'+slacks']
    cols=['nuta','min','max','-slacks cost','+slacks cost','+slacks','NutrDesc']


    flu.sort_values('amounts',inplace=False,ascending=False).to_excel(fluwriter, sheet_name=number,freeze_panes=(2, 2))#.to_excel('flu.xlsx',freeze_panes=(1, 1))
    st[cols].sort_values(col,ascending=False).to_excel(slackswriter,sheet_name=str(i),freeze_panes=(1, 1))#.head(10)#2024-8-24 replaced obj with i for now
    #next 4 lines by chatgpt 2023-12-15
    worksheet = fluwriter.sheets[number]
    worksheet.column_dimensions['B'].width = 50#first_column_width

fluwriter.close();slackswriter.close()

from google.colab import files

# Upload the solution file
uploaded = files.upload()

# Assume the uploaded file is named 'solution.sol'
solution_filename = 'model0.sol'
solution_filename = list(uploaded.keys())[0]  # Get the name of the uploaded file

print("Uploaded files:")
print(uploaded.keys())

with open('results', 'wb') as f:
  pickle.dump(results, f)

from pyomo.environ import ConcreteModel, Var
model = ConcreteModel()
model.x_01001 = Var()
model.y_02002 = Var()
variable_names_mps = {}
#for name in uspyomo.prob.component_map(Var):
for name in model.component_map(Var):

    variable_names_mps[name] = model.component(name).getname()

# Print the mapping between original variable names and adjusted names
for original_name, mps_name in variable_names_mps.items():
    print(f"Original name: {original_name}, Adjusted name in MPS: {mps_name}")

for name in uspyomo.prob.component_map(Var):
    print(uspyomo.prob.component(name).getname()," ",name)

uspyomo.prob.fvars.getname()
uspyomo.prob.component_map(Var)

from pyomo.environ import ConcreteModel, Var

# Define your Pyomo model
model = ConcreteModel()

# Define indexed variables with non-compliant names
model.x = Var([1, 2, 3])
model.y = Var(['a', 'b', 'c'])

# Retrieve the adjusted names as they will appear in the MPS file
indexed_variable_names_mps = {}
for index, var in model.x.items():
    indexed_variable_names_mps[(var, index)] = var.getname()

for index, var in model.y.items():
    indexed_variable_names_mps[(var, index)] = var.getname()

# Print the mapping between original indexed variable names and adjusted names
for (original_var, index), mps_name in indexed_variable_names_mps.items():
    print(f"Original name: {original_var}[{index}], Adjusted name in MPS: {mps_name}")

model.x.set_values

import pickle
OUTPUT = open('abstract7.pyomo','w')
OUTPUT.write(str(pickle.dumps(model)))#pyomo rule is preventing.ttributeError: Can't pickle local object 'UScplex1int.<locals>.ub_rule'
OUTPUT.close()
with open('abstract7.pyomo', 'rb') as f:
    reloaded_model = pickle.load(f)

r=uspyomo.UScplex1int(mixed=False,binconsl=intvits,mfg=None,mn=0,mfgex=mfgex)

r=uspyomo.UScplex1int(mixed=False,binconsl=None,mfgex=[],mn=0)

# cystine and theanine https://pubmed.ncbi.nlm.nih.gov/20145562/

#nzf2=uspyomo.UScplex2(False,timelimit=50)
prob=uspyomo.prob
from pyomo.environ import *
solver = SolverFactory('scip')
solver.options['limits/time'] = 6000
result = solver.solve(prob, tee=True)

print("status:", result.solver.status, result.solver.termination_condition)#,")# obj=",value(prob.objective))
# if result.solver.termination_condition == TerminationCondition.maxTimeLimit:#2025-3-21 doesn't work anymore,skip it
#     # Continue solving with a new time limit

# # Set initial values of variables to previous solution

#     for var in prob.component_data_objects(Var, active=True):
#         var.set_value(result[var])

    #np.set_printoptions(threshold=sys.maxsize)

result = solver.solve(prob, tee=True)

uspyomo.chosenfoods_cplex['amounts']=prob.fvars.get_values()

model_series = pd.Series({idx: value(prob.fvars[idx]) for idx in prob.fvars})#.reindex(uspyomo.chosenfoods_cplex.index)

uspyomo.chosenfoods_cplex['amounts']=model_series.values#.update(model_series.values)#chosenfoods_cplex[['amounts','Long_Desc']])#2022-04-09 Long_desc has "CPLEX" appended

uspyomo.chosenfoods['amounts']

uspyomo.chosenfoods_cplex[uspyomo.chosenfoods_cplex['amounts']>0.001][['Long_Desc','amounts']]

#if vectorvars:

    #    chosenfoods_cplex.amounts = [value(prob.fvars[i]) for i in range(nf)]
    #    model_series = pd.Series({idx: value(model.fvars[idx]) for idx in model.fvars})
    #chosenfoods['amounts']=0.0#chosenfoods.drop('amounts',axis='columns',inplace=True)#2022-02-27
    #chosenfoods['amounts'].update(model_series)#chosenfoods_cplex[['amounts','Long_Desc']])#2022-04-09 Long_desc has "CPLEX" appended
    chosenfoods['amounts']=0.0#chosenfoods.drop('amounts',axis='columns',inplace=True)#2022-02-27
    chosenfoods.update(chosenfoods_cplex[['amounts','Long_Desc']])#2022-04-09 Long_desc has "CPLEX" appended

uspyomo.solver.options['display/verblevel'] = 0

nzf2=uspyomo.UScplex2(False,timelimit=600)

pd.Series( )

r=uspyomo.UScplex1int(mixed=False,binconsl=intvits,mfg=None,mn=13,mfgex=mfgex)#mn=16#this should return the cvxpy.problem to cache i
#Neos time zone is 2 hours later
datetime.now().strftime("%d/%m/%Y %H:%M:%S")
#mixed actually means neos

for index, row in results_df.iterrows():
    variable_name = row['Variable']
    value = row['Value']

    # Update the model variable with the solution value
    if hasattr(uspyomo.prob, variable_name):
        getattr(uspyomo.prob, variable_name).value = value
    else:
        print(f"Variable {variable_name} not found in the model.")

# Display the updated variable values
print("Updated model variable values:")
for var in uspyomo.prob.component_objects(Var, descend_into=True):
    for v in var.values():
        print(f"{v.name} = {v.value}")

import pandas as pd#2022-11-01 if reading gurobi cplex output:
cbcsol=pd.read_table(vd+"model.sol",index_col=0,sep=None,names=['amounts'],dtype={'name':str,'amounts':float},skiprows=0,engine='python',on_bad_lines='skip')#,dtype={'NAME':str,'CVX_xpress_qp':float},
#names=['name','amounts'],dtype={'name':str,'amounts':np.float},skiprows=1



"""on_bad_lines : {'error', 'warn', 'skip'} or callable, default 'error'
        Specifies what to do upon encountering a bad line (a line with too many fields).
        Allowed values are :
    
            - 'error', raise an Exception when a bad line is encountered.
            - 'warn', raise a warning when a bad line is encountered and skip that line.
            - 'skip', skip bad lines without raising or warning when they are encountered.
"""

#  var_sol = cbcsol.reset_index().loc[cbcsol.reset_index().index.str.startswith(var_name)].set_index('index')

ls_vars = cbcsol[cbcsol.index.str.startswith('lsvars')]

uspyomo.chosenfoods_cplex.amounts=ls_vars.amounts.values

uspyomo.chosenfoods_cplex.shape,ls_vars.shape

# prompt: # prompt: Using dataframe ls_vars: strip off the parentheses and index values from the contents of column named index. keep the initial string and put the parenthetic number in a new column (without the parentheses)

ls_vars[['index','idx']] = ls_vars['index'].str.split('(',expand=True)
ls_vars['idx'] = ls_vars['idx'].str.rstrip(')')

# prompt: turn the columns 'index' and 'idx' into an equivalent dictionary

dict(zip(ls_vars['index'], ls_vars['idx']))

zip(ls_vars['index'], ls_vars['idx'])

ls_vars_dicts=ls_vars.set_index('idx')['amounts'].to_dict()

#from here on is pasted from reading neos solution pools and building output spreadsheet
nutmap=uscvxpy.consdf[uscvxpy.consdf.NutrDesc.notnull()].NutrDesc.to_dict()

fl=nzf2[nzf2['amounts']>1e-3][['Long_Desc','amounts','allmeas']]
flu=fl.merge(fl9,left_index=True,right_index=True,how='left')
flu['M']=flu['amounts']/flu['Gm_Wgt']
#flu.to_excel('flu.xlsx',freeze_panes=(1, 1))
flu=fineprint(flu)#add in the food drilldowns as %
#[['Long_Desc','M','Msre_Desc','amounts']]
obj,st=uscvxpy.slackscplex()
col=['-slacks cost','+slacks cost']#,'+slacks']
cols=['nuta','min','max','-slacks cost','+slacks cost','+slacks','NutrDesc']

fluwriter = pd.ExcelWriter(vd+"flu.xlsx", engine="openpyxl")
slackswriter = pd.ExcelWriter(vd+"slacks.xlsx", engine="openpyxl")

flu.sort_values('amounts',inplace=False,ascending=False).to_excel(fluwriter, sheet_name="0",freeze_panes=(2, 2))#.to_excel('flu.xlsx',freeze_panes=(1, 1))
st[cols].sort_values(col,ascending=False).to_excel(slackswriter,sheet_name=str(obj),freeze_panes=(1, 1))#.head(10)
#next 4 lines by chatgpt 2023-12-15

worksheet = fluwriter.sheets['0']
worksheet.column_dimensions['B'].width = 50#first_column_width
fluwriter.close();slackswriter.close()

matrix=uscvxpy.npt.values
import numpy as np
non_zero_matrix = matrix[np.nonzero(matrix)]
min_index = np.argmin(np.abs(non_zero_matrix))# Find the smallest absolute non-zero value
row_index, col_index = np.unravel_index(np.flatnonzero(matrix == non_zero_matrix[min_index]), matrix.shape) #Convert flattened index to row and column indices
print("Position of the smallest absolute non-zero value:", (row_index, col_index))
print("those are: ",uscvxpy.npt.index[row_index]," and ", uscvxpy.npt.columns[col_index])
max_index = np.argmax(np.abs(non_zero_matrix))# Find the largestest absolute non-zero value
row_index, col_index = np.unravel_index(np.flatnonzero(matrix == non_zero_matrix[max_index]), matrix.shape)# Convert flattened index to row and column indices
print("Position of the largest absolute non-zero value:", (row_index, col_index))
print("those are: ",uscvxpy.npt.index[row_index]," and ", uscvxpy.npt.columns[col_index])
import numpy as np
matrix = uscvxpy.npt.values
non_zero_matrix = matrix[np.nonzero(matrix)]# Mask zero values
smallest_abs_non_zero = np.min(np.abs(non_zero_matrix))# Find the smallest absolute non-zero value
print("Smallest absolute non-zero value:", smallest_abs_non_zero,"largest ",np.max(np.abs(non_zero_matrix)))
import numpy as np
matrix = uscvxpy.npt.values
non_zero_elements = matrix[np.nonzero(matrix)]# Mask zero values
# Initialize a list to store the smallest values and their positions
smallest_values = [];smallest_positions = []
for _ in range(min(10, len(non_zero_elements))):# Find the 10 smallest absolute non-zero values and their positions
    min_index = np.argmin(np.abs(non_zero_elements))# Find the index of the smallest absolute non-zero value
    min_value = non_zero_elements[min_index]# Get the value and position of the smallest absolute non-zero value
    row_index, col_index = np.unravel_index(np.flatnonzero(matrix == min_value), matrix.shape)
    smallest_values.append(min_value)    #smallest_positions.append((row_index, col_index))
    smallest_positions.append((uscvxpy.npt.index[row_index], uscvxpy.npt.columns[col_index]))
    non_zero_elements = np.delete(non_zero_elements, min_index)# Exclude the smallest value from consideration
for i, (value, position) in enumerate(zip(smallest_values, smallest_positions), 1):# Print the smallest absolute non-zero values and their positions
    print(f"{i}. Value: {value}, Position: {position}")

uscvxpy.UScplex2(True)

import pickle
with open('cvxpy_problem.pickle', 'wb') as f:
    pickle.dump(nzf2, f)

uscvxpy.inverse_data.var_shapes

#makes sure these are enabled in chosenfoods and vice versa
intvits=['A0000','A0001','A0002','A0003','A0005','A0006','A0009','A0010','A0015','A0017','A0018','A0025','A0026',"A0029"]#,'A0013','A0012''A0027']#,'A0016','A0019''A0011',,'A0007','A0008','A0004','A0023'] #etc. could also make a column in chosenfoods1, but ..
mfgex=intvits+['04531','04589','42231','14355','14260','A0020','A0025','11593','A0028',"02028"]#not counted in the 14 (because of low storage/prep cost)
#soybean leithin,cod liver oil, flaseed oil,black tea,decaf green tea,bcaa,tmg,raw waxgourd,potas chloride,paprika
#A0026 is selenium, should it be exclude, or be counted as food ? because of the variability of food source and the consequences of both excess and insufficiency and lack of body storage? 2023-10-09
#lecithin caps, cod liver oil caps;flax oil caps;black tea; just pass the lists into UScplex1...

intvits=list(zip(intvits,[1]*len(intvits)))#+fl7 #2023-03-29 fl7 is the measures quantization#+intfoods 2022-12-26 fl6 from spreadsheet now, todo put those in multiindex and get rid of the list append here
r=uscvxpy.UScplex1int(mixed=False,binconsl=intvits,mfg=None,mn=13,mfgex=mfgex)#mn=16#this should return the cvxpy.problem to cache i
#Neos time zone is 2 hours later
datetime.now().strftime("%d/%m/%Y %H:%M:%S")
#mixed actually means neos

intvits=None#use this cell instead of the next one for pure linear (fast)
intvits=['A0000','A0001','A0002','A0003','A0005','A0006','A0009','A0010','A0015','A0017','A0018','A0025','A0026',"A0029","A0030"]#,'A0013','A0012''A0027']#,'A0016','A0019''A0011',,'A0007','A0008','A0004','A0023'] #etc. could also make a column in chosenfoods1, but ..
intvits=list(zip(intvits,[1]*len(intvits)))
mfgex=[]
#mn has to be zero
r=uscvxpy.UScplex1int(mixed=False,binconsl=None,mfgex=[],mn=20)#mixedFalse means going to solve locally using UScplex2local (i.e. no solve_via_data(and also no solution pool))
#Neos time zone is 2 hours later
datetime.now().strftime("%d/%m/%Y %H:%M:%S")
#mixed actually means neos
nzf2=uscvxpy.UScplex2(False)
#from here on is pasted from reading neos solution pools and building output spreadsheet
nutmap=uscvxpy.consdf[uscvxpy.consdf.NutrDesc.notnull()].NutrDesc.to_dict()

fl=nzf2[nzf2['amounts']>1e-3][['Long_Desc','amounts','allmeas']]
flu=fl.merge(fl9,left_index=True,right_index=True,how='left')
flu['M']=flu['amounts']/flu['Gm_Wgt']
#flu.to_excel('flu.xlsx',freeze_panes=(1, 1))
flu=fineprint(flu)#add in the food drilldowns as %
#[['Long_Desc','M','Msre_Desc','amounts']]
obj,st=uscvxpy.slackscplex()
col=['-slacks cost','+slacks cost']#,'+slacks']
cols=['nuta','min','max','-slacks cost','+slacks cost','+slacks','NutrDesc']

fluwriter = pd.ExcelWriter(vd+"flu.xlsx", engine="openpyxl")
slackswriter = pd.ExcelWriter(vd+"slacks.xlsx", engine="openpyxl")

flu.sort_values('amounts',inplace=False,ascending=False).to_excel(fluwriter, sheet_name="0",freeze_panes=(2, 2))#.to_excel('flu.xlsx',freeze_panes=(1, 1))
st[cols].sort_values(col,ascending=False).to_excel(slackswriter,sheet_name=str(obj),freeze_panes=(1, 1))#.head(10)
#next 4 lines by chatgpt 2023-12-15

worksheet = fluwriter.sheets['0']
worksheet.column_dimensions['B'].width = 50#first_column_width
fluwriter.close();slackswriter.close()

import pandas as pd; import numpy as np
#cbcsol2=pd.read_fwf("/mnt/c/Users/Rwsin/myproject/model.sol")
cbcsol=pd.read_table("c:/Users/Rwsin/myproject/model.sol",sep=None,names=['name','amounts'],dtype={'name':str,'amounts':np.float},skiprows=1,)

"""# xml-rpc"""



# ============================================
# NEOS AUTOMATION FOR GOOGLE COLAB
# Simple, flattened version - pick one method and go
# ============================================

# --------------------------------------------
# METHOD 1: Easiest - google-colab-selenium
# --------------------------------------------
# Run this cell once per session:
!pip install google-colab-selenium
#pip install google-colab-selenium

# Configuration
MPS_FILE = '/content/drive/MyDrive/cplex.mps'           # Required: path to your MPS file
PARAM_FILE = None                      # Optional: path to parameter file
EMAIL = 'rwsinnovations@gmail.com'               # Required
SHORT_PRIORITY = False                 # True = 5 min max, False = 8 hour max
RETURN_SOL_FILE = True                 # Return .sol file in results
RETURN_MST_FILE = False                # Return .mst file (MIP start)
RETURN_BAS_FILE = False                # Return .bas file (basis, LP only)

# ============================================
# NEOS GUROBI MPS SUBMISSION - MORE ROBUST
# ============================================

# Run this ONCE per Colab session (in a separate cell):
# !pip install google-colab-selenium

import google_colab_selenium as gs
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import time
import re
import os

# Configuration
# MPS_FILE = 'my_problem.mps'           # Required: path to your MPS file
# PARAM_FILE = None                      # Optional: path to parameter file
# EMAIL = 'your@email.com'               # Required
# SHORT_PRIORITY = False                 # True = 5 min max, False = 8 hour max
# RETURN_SOL_FILE = True                 # Return .sol file in results
# RETURN_MST_FILE = False                # Return .mst file (MIP start)
# RETURN_BAS_FILE = False                # Return .bas file (basis, LP only)

# Get absolute path
MPS_FILE = os.path.abspath(MPS_FILE)
if PARAM_FILE:
    PARAM_FILE = os.path.abspath(PARAM_FILE)

# Create driver
print("Starting Chrome...")
driver = gs.Chrome()
driver.set_page_load_timeout(60)

# Go to Gurobi MPS page
print("Loading Gurobi MPS submission page...")
driver.get('https://neos-server.org/neos/solvers/milp:Gurobi/MPS.html')

# Give page time to load
time.sleep(5)

wait = WebDriverWait(driver, 30)

# Wait for form
print("Waiting for form to load...")
wait.until(EC.presence_of_element_located((By.NAME, "field.1")))
time.sleep(2)

# Upload MPS file (field.1)
print(f"Uploading MPS file: {MPS_FILE}")
mps_input = driver.find_element(By.NAME, "field.1")
mps_input.send_keys(MPS_FILE)
time.sleep(1)

# Upload parameter file (field.2) - optional
if PARAM_FILE:
    print(f"Uploading parameter file: {PARAM_FILE}")
    param_input = driver.find_element(By.NAME, "field.2")
    param_input.send_keys(PARAM_FILE)
    time.sleep(1)

# Checkboxes for return files
if RETURN_BAS_FILE:
    bas_checkbox = driver.find_element(By.NAME, "field.6")
    bas_checkbox.click()

if RETURN_MST_FILE:
    mst_checkbox = driver.find_element(By.NAME, "field.7")
    mst_checkbox.click()

if RETURN_SOL_FILE:
    sol_checkbox = driver.find_element(By.NAME, "field.8")
    sol_checkbox.click()

# Short priority checkbox
if SHORT_PRIORITY:
    priority_checkbox = driver.find_element(By.NAME, "priority")
    priority_checkbox.click()

# Email (required)
print("Filling email...")
email_input = driver.find_element(By.NAME, "email")
email_input.send_keys(EMAIL)

# Submit
print("Submitting job...")
driver.set_page_load_timeout(120)  # 2 minutes for submission
submit_button = driver.find_element(By.CSS_SELECTOR, "input[type='submit'][value='Submit to NEOS']")
submit_button.click()

# Wait for results page (can take a while)
print("Waiting for response (this may take up to 2 minutes)...")
wait = WebDriverWait(driver, 120)
wait.until(EC.presence_of_element_located((By.TAG_NAME, "pre")))

# Get page text
results_text = driver.find_element(By.TAG_NAME, "body").text

# Extract job number and password
job_match = re.search(r'Job#\s*:\s*(\d+)', results_text)
if not job_match:
    job_match = re.search(r'Job #(\d+)', results_text)

pass_match = re.search(r'Password\s*:\s*(\S+)', results_text)
if not pass_match:
    pass_match = re.search(r'password:\s*(\S+)', results_text)

job_number = job_match.group(1) if job_match else "Unknown"
password = pass_match.group(1) if pass_match else "Unknown"

# Determine if results are immediate or delayed
if "Job completed" in results_text and "awaiting results" in results_text:
    result_type = "DELAYED"
    print(f"\nâœ“ Job submitted and queued!")
    print(f"Job Number: {job_number}")
    print(f"Password: {password}")
    print(f"Status: Job will run on NEOS cluster - results will be emailed")
elif "Queued:" in results_text:
    result_type = "QUEUED"
    print(f"\nâœ“ Job submitted!")
    print(f"Job Number: {job_number}")
    print(f"Password: {password}")
    print(f"Status: Job is queued - results will be emailed when complete")
elif "Solution" in results_text or "optimal" in results_text.lower():
    result_type = "IMMEDIATE"
    print(f"\nâœ“ Job submitted and completed immediately!")
    print(f"Job Number: {job_number}")
    print(f"Password: {password}")
    print(f"Status: Results are available now (also saved to file)")
else:
    result_type = "UNKNOWN"
    print(f"\nâœ“ Job submitted!")
    print(f"Job Number: {job_number}")
    print(f"Password: {password}")
    print(f"Status: Unknown - check neos_job_info.txt")

# Save job info
with open('neos_job_info.txt', 'w') as f:
    f.write(f"Job Number: {job_number}\n")
    f.write(f"Password: {password}\n")
    f.write(f"Result Type: {result_type}\n")
    f.write(f"\n{results_text}")

driver.quit()

print(f"\nâœ“ Job info saved to neos_job_info.txt")

# Return job info for use in next steps
job_info = {
    'job_number': job_number,
    'password': password,
    'result_type': result_type
}

# Run this ONCE per Colab session (in a separate cell):
# !pip install google-colab-selenium

import google_colab_selenium as gs
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import imaplib
import email
import re
import os
import time
from google.colab import drive

# --- CONFIGURATION ---
EMAIL_USER = 'rwsinnovations@gmail.com'
EMAIL_PROVIDER = 'gmail'
EMAIL_PASS = "odmm dswr yobj ciob"
SEARCH_CRITERIA = '(FROM "no-reply@neos-server.org")'
DRIVE_FOLDER = '/content/drive/MyDrive/neos_results'
# ---------------------

drive.mount('/content/drive')

def connect_to_mail(provider, user, password):
    host = "imap.gmail.com" if provider == 'gmail' else "imap.mail.yahoo.com"
    mail = imaplib.IMAP4_SSL(host)
    mail.login(user, password.strip())
    return mail

def get_neos_credentials(text):
    id_match = re.search(r"Job Number=(\d+)", text)
    pass_match = re.search(r"Password=(\S+)", text)
    return (id_match.group(1) if id_match else None,
            pass_match.group(1) if pass_match else None)

def fetch_result_from_web(driver, job_id, password):
    url = "https://neos-server.org/neos/admin.html"
    print(f"   âž¤ Browsing: {url}")

    driver.set_page_load_timeout(60)
    driver.get(url)

    # Give page time to fully load
    time.sleep(3)

    wait = WebDriverWait(driver, 30)

    # Fill Job Number
    print(f"   âž¤ Filling job number: {job_id}")
    job_input = wait.until(EC.presence_of_element_located((By.NAME, "jobnumber")))
    job_input.clear()
    job_input.send_keys(job_id)
    time.sleep(0.5)

    # Fill Password
    print(f"   âž¤ Filling password")
    pass_input = driver.find_element(By.NAME, "pass")
    pass_input.clear()
    pass_input.send_keys(password)
    time.sleep(0.5)

    # Select "View Job Results" radio button
    print(f"   âž¤ Selecting results option")
    results_radio = driver.find_element(By.ID, "admin_results")
    results_radio.click()
    time.sleep(0.5)

    # Click Submit
    print(f"   âž¤ Submitting form")
    submit_btn = driver.find_element(By.CSS_SELECTOR, "input[type='submit'][value='Submit']")
    submit_btn.click()

    # Wait for results page
    print("   âž¤ Waiting for results page to load...")
    time.sleep(2)
    wait.until(EC.presence_of_element_located((By.TAG_NAME, "pre")))

    # Get results
    results_text = driver.find_element(By.TAG_NAME, "body").text

    return results_text

# --- MAIN EXECUTION ---

# 1. Login to Email
print(f"Connecting to {EMAIL_USER}...")

mail = connect_to_mail(EMAIL_PROVIDER, EMAIL_USER, EMAIL_PASS)
mail.select("INBOX")

status, messages = mail.search(None, SEARCH_CRITERIA)
email_ids = messages[0].split()
print(f"Found {len(email_ids)} result emails.")

if email_ids:
    # 2. Start Driver (Only once)
    print("Starting Chrome Driver...")
    driver = gs.Chrome()

    # Process latest 3 emails
    for i in email_ids[-3:]:
        res, msg_data = mail.fetch(i, "(RFC822)")
        msg = email.message_from_bytes(msg_data[0][1])

        body = msg.get_payload(decode=True).decode() if not msg.is_multipart() else \
               next(p.get_payload(decode=True).decode() for p in msg.walk() if p.get_content_type() == "text/plain")

        job_id, job_pass = get_neos_credentials(body)

        if job_id and job_pass:
            print(f"\n--- Processing Job {job_id} ---")

            # 3. Fetch from Web
            results_text = fetch_result_from_web(driver, job_id, job_pass)

            # 4. Save to Drive
            output_path = f'{DRIVE_FOLDER}/job_{job_id}.txt'
            os.makedirs(os.path.dirname(output_path), exist_ok=True)

            with open(output_path, 'w') as f:
                f.write(results_text)

            print(f"   âœ“ Saved to: {output_path}")
        else:
            print(f"   âœ— Could not extract credentials from email")

    driver.quit()
    print("\nDriver closed.")
else:
    print("No emails found.")

mail.close()
mail.logout()

print("\nâœ“ Done!")

job_number, password

# ============================================
# NEOS JOB STATUS CHECK - MORE ROBUST
# ============================================

#!pip install google-colab-selenium

import google_colab_selenium as gs
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import time

# Configuration
job_number='18072050'
password='JNEmrxBT'
JOB_NUMBER=job_number# = "12345678"
PASSWORD=password# = "abc123XYZ"

# Create driver with longer timeout
driver = gs.Chrome()
driver.set_page_load_timeout(30)

# Go to admin page
print("Loading admin page...")
driver.get('https://neos-server.org/neos/admin.html')

# Give page extra time to load
time.sleep(3)

wait = WebDriverWait(driver, 20)

# Fill job number
print("Filling form...")
job_number_field = wait.until(EC.presence_of_element_located((By.NAME, "jobnumber")))
job_number_field.clear()
job_number_field.send_keys(JOB_NUMBER)

# Fill password
password_field = driver.find_element(By.NAME, "pass")
password_field.clear()
password_field.send_keys(PASSWORD)

# Select "View Job Queue" radio button
queue_radio = driver.find_element(By.ID, "admin_queue")
queue_radio.click()

# Click Submit
print("Submitting...")
submit_button = driver.find_element(By.CSS_SELECTOR, "input[type='submit']")
submit_button.click()

# Wait for results
time.sleep(2)
wait.until(EC.presence_of_element_located((By.TAG_NAME, "body")))

# Get status text
status_text = driver.find_element(By.TAG_NAME, "body").text

# Print status
print(status_text)

# Save to file
with open('neos_status.txt', 'w') as f:
    f.write(status_text)

driver.quit()

print(f"\nâœ“ Status checked for job {JOB_NUMBER}")
print(f"âœ“ Saved to neos_status.txt")

# ============================================
# NEOS JOB STATUS CHECK - USING RESULTS ENDPOINT
# ============================================

# Run this ONCE per Colab session (in a separate cell):
# !pip install google-colab-selenium

import google_colab_selenium as gs
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import time

# Configuration
JOB_NUMBER = job_number#"12345678"
PASSWORD = password#"abc123XYZ"

# Create driver
driver = gs.Chrome()
driver.set_page_load_timeout(30)

# Go to admin page
print("Loading admin page...")
driver.get('https://neos-server.org/neos/admin.html')
time.sleep(3)

wait = WebDriverWait(driver, 20)

# Fill job number
print("Filling form...")
job_number_field = wait.until(EC.presence_of_element_located((By.NAME, "jobnumber")))
job_number_field.clear()
job_number_field.send_keys(JOB_NUMBER)

# Fill password
password_field = driver.find_element(By.NAME, "pass")
password_field.clear()
password_field.send_keys(PASSWORD)

# Select "View Job Results" - this will show status if not done
results_radio = driver.find_element(By.ID, "admin_results")
results_radio.click()

# Click Submit
print("Checking status...")
submit_button = driver.find_element(By.CSS_SELECTOR, "input[type='submit']")
submit_button.click()

# Wait for response
time.sleep(2)
wait.until(EC.presence_of_element_located((By.TAG_NAME, "body")))

# Get response text
response_text = driver.find_element(By.TAG_NAME, "body").text

# Determine status
if "Incorrect password" in response_text:
    status = "ERROR: Incorrect password"
elif "Job not found" in response_text or "does not exist" in response_text:
    status = "ERROR: Job not found"
elif "Running:" in response_text and JOB_NUMBER in response_text:
    status = "RUNNING"
elif "Queued:" in response_text and JOB_NUMBER in response_text:
    status = "QUEUED"
elif "Job completed" in response_text or "awaiting results" in response_text:
    status = "RUNNING (awaiting results)"
elif "optimal" in response_text.lower() or "solution" in response_text.lower():
    status = "DONE (results available)"
else:
    status = "UNKNOWN"

print(f"\nJob {JOB_NUMBER} Status: {status}")
print(f"\nFull response:")
print(response_text[:1000])  # First 1000 chars

# Save to file
with open('neos_status.txt', 'w') as f:
    f.write(f"Job: {JOB_NUMBER}\n")
    f.write(f"Status: {status}\n\n")
    f.write(response_text)

driver.quit()

print(f"\nâœ“ Status saved to neos_status.txt")

# ============================================
# NEOS JOB STATUS CHECK - USING RESULTS ENDPOINT
# ============================================

# Run this ONCE per Colab session (in a separate cell):
# !pip install google-colab-selenium

import google_colab_selenium as gs
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import time

# Configuration
# JOB_NUMBER = "12345678"
# PASSWORD = "abc123XYZ"

# Create driver
driver = gs.Chrome()
driver.set_page_load_timeout(30)

# Go to admin page
print("Loading admin page...")
driver.get('https://neos-server.org/neos/admin.html')
time.sleep(3)

wait = WebDriverWait(driver, 20)

# Fill job number
print("Filling form...")
job_number_field = wait.until(EC.presence_of_element_located((By.NAME, "jobnumber")))
job_number_field.clear()
job_number_field.send_keys(JOB_NUMBER)

# Fill password
password_field = driver.find_element(By.NAME, "pass")
password_field.clear()
password_field.send_keys(PASSWORD)

# Select "View Job Results" - this will show status if not done
results_radio = driver.find_element(By.ID, "admin_results")
results_radio.click()

# Click Submit
print("Checking status...")
submit_button = driver.find_element(By.CSS_SELECTOR, "input[type='submit']")
submit_button.click()

# Wait for response
time.sleep(2)
wait.until(EC.presence_of_element_located((By.TAG_NAME, "body")))

# Get response text
response_text = driver.find_element(By.TAG_NAME, "body").text

# Determine status
if "Incorrect password" in response_text:
    status = "ERROR: Incorrect password"
    output_file = None
elif "Job not found" in response_text or "does not exist" in response_text:
    status = "ERROR: Job not found"
    output_file = None
elif "Running:" in response_text and JOB_NUMBER in response_text:
    status = "RUNNING"
    output_file = None
elif "Queued:" in response_text and JOB_NUMBER in response_text:
    status = "QUEUED"
    output_file = None
elif "Job completed" in response_text or "awaiting results" in response_text:
    status = "RUNNING (awaiting results)"
    output_file = None
elif "optimal" in response_text.lower() or "solution" in response_text.lower():
    status = "DONE (results available)"
    # Extract output file if present
    import re
    output_match = re.search(r'Additional Output:\s*(\S+\.zip)', response_text)
    output_file = output_match.group(1) if output_match else None
else:
    status = "UNKNOWN"
    output_file = None

print(f"\nJob {JOB_NUMBER} Status: {status}")
if output_file:
    print(f"Output file: {output_file}")
    print(f"Download URL: https://neos-server.org/neos/jobs/{output_file}")

print(f"\nFull response:")
print(response_text[:1000])  # First 1000 chars

# Save to file
with open('neos_status.txt', 'w') as f:
    f.write(f"Job: {JOB_NUMBER}\n")
    f.write(f"Status: {status}\n")
    if output_file:
        f.write(f"Output File: {output_file}\n")
        f.write(f"Download URL: https://neos-server.org/neos/jobs/{output_file}\n")
    f.write(f"\n{response_text}")

# Download output file if available
if output_file:
    # Try to find a download link on the page
    print(f"\nLooking for download link for {output_file}...")

    # Look for links with the output filename
    from selenium.webdriver.common.by import By
    links = driver.find_elements(By.TAG_NAME, "a")

    download_link = None
    for link in links:
        href = link.get_attribute("href")
        if href and output_file in href:
            download_link = href
            print(f"Found link: {download_link}")
            break

    if download_link:
        import requests
        print(f"Downloading from: {download_link}")
        response = requests.get(download_link)

        if response.status_code == 200:
            with open(output_file, 'wb') as f:
                f.write(response.content)
            print(f"âœ“ Downloaded to: {output_file}")
        else:
            print(f"âœ— Download failed: HTTP {response.status_code}")
    else:
        print(f"âœ— Could not find download link for {output_file}")

driver.quit()

print(f"\nâœ“ Status saved to neos_status.txt")

"""# old neos api attempts"""

# --------------------------------------------
# METHOD 2: Backup - chromedriver-autoinstaller
# --------------------------------------------
# Use this if Method 1 doesn't work
# Run this cell once per session:
!pip install selenium chromedriver-autoinstaller

mps_file_path = "G:/My Drive/US/oxalate/mgca1to1/b6/glutamic/homeless/shelterfood/bipolar ratios/arginine/methionine/water/mealcycling/sols/cplex.mps"  # Updated file path
mps_file_path = vd+"cplex.mps"#"G:\\My Drive\\US\\oxalate\\mgca1to1\\b6\\glutamic\\homeless\\shelterfood\\bipolar ratios\\arginine\\methionine\\water\\mealcycling\\sols\\cplex.mps"
mps_file_path = vd+"test.mps"#"G:\\My Drive\\US\\oxalate\\mgca1to1\\b6\\glutamic\\homeless\\shelterfood\\bipolar ratios\\arginine\\methionine\\water\\mealcycling\\sols\\cplex.mps"

# Read the contents of the "cplex.mps" file

with open(mps_file_path, "r", encoding="ISO-8859-1") as mps_file:
  mps_content = mps_file.read()

mps_content

neos.getSolverTemplate("milp","SCIP","MPS")

neos.listAllSolvers()	#Returns a list of all solvers available on NEOS, formated as category:solver:inputMethod

neos.listCategories()



solver = "scip"#"CPLEX"
import xmlrpc.client
import time
import zipfile
import os
import pickle
neos = xmlrpc.client.ServerProxy("https://neos-server.org:3333")
category = "milp"

input_method = "MPS"
email = "rwsinnovations@gmail.com"  # Updated email address
# Leave the "post," "basis," "MST," "SOL," and "comments" fields empty
post_content = "";bas_content = "";mst_content = ""
sol_content = "";options_content="";comments = ""

# Set the flags to include the desired files in the results
wantbas = False;wantmst = False;wantsol = True

# Create the XML message for submission
xml_message = (
    "<document>\n"
    "<category>{0}</category>\n"
    "<solver>{1}</solver>\n"
    "<inputMethod>{2}</inputMethod>\n"
    "<email>{3}</email>\n"
    "<mps><![CDATA[{4}]]></mps>\n"
    "<options><![CDATA[{5}]]></options>\n"
    "<post><![CDATA[{6}]]></post>\n"
    "<BAS><![CDATA[{7}]]></BAS>\n"
    "<MST><![CDATA[{8}]]></MST>\n"
    "<SOL><![CDATA[{9}]]></SOL>\n"
    "<wantbas><![CDATA[{10}]]></wantbas>\n"
    "<wantmst><![CDATA[{11}]]></wantmst>\n"
    "<wantsol><![CDATA[{12}]]></wantsol>\n"
    "<comments><![CDATA[{13}]]></comments>\n"
    "</document>"
).format(category, solver, input_method, email, mps_content, options_content, post_content, bas_content, mst_content, sol_content, wantbas, wantmst, wantsol, comments)
jobNumber, password = neos.submitJob(xml_message)
print(f"Job number = {jobNumber}\nJob password = {password}")

neos.getSolverTemplate("milp","CPLEX","MPS")

import xmlrpc.client
import time
import zipfile
import os
import pickle
neos = xmlrpc.client.ServerProxy("https://neos-server.org:3333")
category = "milp"
solver = "CPLEX"
input_method = "MPS"
email = "rwsinnovations@gmail.com"  # Updated email address
# Leave the "post," "basis," "MST," "SOL," and "comments" fields empty
post_content = "";bas_content = "";mst_content = ""
sol_content = "";options_content="";comments = ""

# Set the flags to include the desired files in the results
wantbas = False;wantmst = False;wantsol = True

# Create the XML message for submission
xml_message = (
    "<document>\n"
    "<category>{0}</category>\n"
    "<solver>{1}</solver>\n"
    "<inputMethod>{2}</inputMethod>\n"
    "<email>{3}</email>\n"
    "<MPS><![CDATA[{4}]]></MPS>\n"
    "<options><![CDATA[{5}]]></options>\n"
    "<post><![CDATA[{6}]]></post>\n"
    "<BAS><![CDATA[{7}]]></BAS>\n"
    "<MST><![CDATA[{8}]]></MST>\n"
    "<SOL><![CDATA[{9}]]></SOL>\n"
    "<wantbas><![CDATA[{10}]]></wantbas>\n"
    "<wantmst><![CDATA[{11}]]></wantmst>\n"
    "<wantsol><![CDATA[{12}]]></wantsol>\n"
    "<comments><![CDATA[{13}]]></comments>\n"
    "</document>"
).format(category, solver, input_method, email, mps_content, options_content, post_content, bas_content, mst_content, sol_content, wantbas, wantmst, wantsol, comments)
jobNumber, password = neos.submitJob(xml_message)
print(f"Job number = {jobNumber}\nJob password = {password}")

#'milp:HiGHS:MPS'
neos.getSolverTemplate("milp","HiGHS","MPS")



solver = "HiGHS"
import xmlrpc.client
import time
import zipfile
import os
import pickle
neos = xmlrpc.client.ServerProxy("https://neos-server.org:3333")
category = "milp"

input_method = "MPS"
email = "rwsinnovations@gmail.com"  # Updated email address
# Leave the "post," "basis," "MST," "SOL," and "comments" fields empty
post_content = "";bas_content = "";mst_content = ""
sol_content = "";options_content="time_limit 28800";comments = ""

# Set the flags to include the desired files in the results
wantbas = False;wantmst = False;wantsol = True

# Create the XML message for submission
xml_message = (
    "<document>\n"
    "<category>{0}</category>\n"
    "<solver>{1}</solver>\n"
    "<inputMethod>{2}</inputMethod>\n"
    "<email>{3}</email>\n"
    "<MPS><![CDATA[{4}]]></MPS>\n"
    "<options><![CDATA[{5}]]></options>\n"
    "<post><![CDATA[{6}]]></post>\n"
    "<BAS><![CDATA[{7}]]></BAS>\n"
    "<MST><![CDATA[{8}]]></MST>\n"
    "<SOL><![CDATA[{9}]]></SOL>\n"
    "<wantbas><![CDATA[{10}]]></wantbas>\n"
    "<wantmst><![CDATA[{11}]]></wantmst>\n"
    "<wantsol><![CDATA[{12}]]></wantsol>\n"
    "<comments><![CDATA[{13}]]></comments>\n"
    "</document>"
).format(category, solver, input_method, email, mps_content, options_content, post_content, bas_content, mst_content, sol_content, wantbas, wantmst, wantsol, comments)
jobNumber, password = neos.submitJob(xml_message)
print(f"Job number = {jobNumber}\nJob password = {password}")

# Save job information to a pickle file
job_info = {"jobNumber": jobNumber, "password": password}
with open("job_status.pkl", "wb") as f:
    pickle.dump(job_info, f)

import xmlrpc.client
import time
import zipfile
import os
import pickle
with open("job_status.pkl", "rb") as f:
  job_info = pickle.load(f)
  jobNumber, password = job_info["jobNumber"], job_info["password"]

neos = xmlrpc.client.ServerProxy("https://neos-server.org:3333")
neos.getJobStatus(jobNumber, password)

neos.getSolverTemplate("milp","GUROBI","MPS")

# Provide the full path to the options file
options_file_path = vd+"TimeLimit2880.txt"

# Read the contents of the options file
with open(options_file_path, "r") as options_file:
    options_content = options_file.read()

options_content

solver = "Gurobi"
import xmlrpc.client
import time
import zipfile
import os
import pickle
neos = xmlrpc.client.ServerProxy("https://neos-server.org:3333")
category = "milp"

input_method = "MPS"
email = "rwsinnovations@gmail.com"  # Updated email address
# Leave the "post," "basis," "MST," "SOL," and "comments" fields empty
post_content = "";bas_content = "";mst_content = ""
sol_content = "";
options_content="TimeLimit 28800\nOptimalityTol 1e-8\nMIPFocus 3\nScaleFlag 2\nNumericFocus 2\nSolutionNumber 20"
comments = ""

# Set the flags to include the desired files in the results
wantbas = False;wantmst = False;wantsol = True

# Create the XML message for submission
xml_message = (
    "<document>\n"
    "<category>{0}</category>\n"
    "<solver>{1}</solver>\n"
    "<inputMethod>{2}</inputMethod>\n"
    "<email>{3}</email>\n"
    "<MPS><![CDATA[{4}]]></MPS>\n"
    "<options><![CDATA[{5}]]></options>\n"
    "<post><![CDATA[{6}]]></post>\n"
    "<BAS><![CDATA[{7}]]></BAS>\n"
    "<MST><![CDATA[{8}]]></MST>\n"
    "<SOL><![CDATA[{9}]]></SOL>\n"
    "<wantbas><![CDATA[{10}]]></wantbas>\n"
    "<wantmst><![CDATA[{11}]]></wantmst>\n"
    "<wantsol><![CDATA[{12}]]></wantsol>\n"
    "<comments><![CDATA[{13}]]></comments>\n"
    "</document>"
).format(category, solver, input_method, email, mps_content, options_content, post_content, bas_content, mst_content, sol_content, wantbas, wantmst, wantsol, comments)
jobNumber, password = neos.submitJob(xml_message)
print(f"Job number = {jobNumber}\nJob password = {password}")

# Check the job status every 1 minute
while True:
    time.sleep(60)  # Sleep for 60 seconds (1 minute)
    status = neos.getJobStatus(jobNumber, password)
    if status == "Done":
        break

# Retrieve the results once the job is done
result_msg = neos.getFinalResults(jobNumber, password)
result_data = result_msg.data

result_data

result_msg.data.decode('utf-8')

jobNumber='13957723'; password='DPxbHpXr'

r=neos.getOutputFile(jobNumber, password, "solver-output.zip")#status crap is separted out

r=neos.getOutputFile(jobNumber, password, "job.results")#status crap is separted out

"""Retrieve a named output file from NEOS where fileName is one of the following: 'results.txt', 'ampl.sol', 'solver-output.zip', 'job.in', 'job.out', 'job.results'. Note: the existence of a given file may depend on the solver or options selected when the job is started. If the job is still running, then this function will block until the job is finished.

This function will return a base-64 encoded object, which may be an error message if the requested file does not exist or if an illegal fileName argument is passed. Please read your XML-RPC client documentation for decoding a successful file return.
"""

from io import BytesIO
from zipfile import ZipFile
from urllib.request import urlopen
# or: requests.get(url).content

#resp = urlopen("http://www.test.com/file.zip")
myzip = ZipFile(BytesIO(r.data))
myzip.namelist()
#highs doesnt have a zipfile, email elizabeth about it...but neos has size limit, might as well use gurobi and get solution pool too.

r.data

for line in myzip.open('soln.sol').readlines():
    print(line.decode('utf-8'))

"""# myokit"""

!pip install myokit
!apt-get install libsundials-dev

import myokit
from google.colab import drive
drive.mount('/content/drive')

import myokit.formats
i = myokit.formats.importer('cellml')
model=i.model('/content/drive/MyDrive/ten_tusscher_model_2006_epi.cellml')
print(model)

# Define a pacing protocol (stimulus)
protocol = myokit.pacing.blocktrain(duration=500, offset=10, level=1, period=1000)

# Create a simulation object with the protocol
sim = myokit.Simulation(model, protocol)

# Modify electrolyte levels (potassium, calcium, sodium)
model.get('potassium_dynamics.K_o').set_rhs(5.4)  # Normal potassium
model.get('calcium_dynamics.Ca_o').set_rhs(2.0)  # Normal calcium
model.get('sodium_dynamics.Na_o').set_rhs(140)  # Normal sodium

# Run the simulation for 1000 ms
sim.pre(1000) # Prepace the model
# Remove 'engine.time' from the log list
datalog = sim.run(1000, log=['environment.time', 'membrane.V']) # Run the simulation and log the desired variables

# Extract time and membrane potential (voltage)
time = datalog.time() # Use datalog.time() to access simulation time
voltage = datalog['membrane.V']

# Plot the results
import matplotlib.pyplot as plt
plt.plot(time, voltage)
plt.xlabel('Time (ms)')
plt.ylabel('Membrane Potential (mV)')
plt.title('Simulated Action Potential')
plt.show()

# Print components and their variables
for component in model.components():
  print(f"Component: {component.name()}")
  for variable in component.variables():
      print(f"  - {variable.name()}")



"""# get neos status from gmail

"""

import imaplib
import email
from email.header import decode_header
import getpass
import re  # Added for pattern matching

# --- CONFIGURATION ---
EMAIL_PROVIDER = 'gmail'
EMAIL_USER = 'rwsinnovations@gmail.com'
SEARCH_CRITERIA = '(FROM "no-reply@neos-server.org")' # Best for NEOS
# ---------------------

def connect_to_mail(provider, user, password):
    host = "imap.gmail.com" if provider == 'gmail' else "imap.mail.yahoo.com"
    mail = imaplib.IMAP4_SSL(host)
    mail.login(user, "odmm dswr yobj ciob")
    return mail

def extract_neos_data(text):
    # Regex to find Job Number (e.g., "Job 12345" or "Job #: 12345")
    job_id_match = re.search(r"Job\s*(?:#|number|id)?\s*[:.]?\s*(\d+)", text, re.IGNORECASE)
    job_id = job_id_match.group(1) if job_id_match else "Unknown"

    # Simple keyword check for status
    status = "Unknown"
    text_lower = text.lower()
    if "finished" in text_lower:
        status = "FINISHED"
    elif "submitted" in text_lower or "received" in text_lower:
        status = "SUBMITTED"
    elif "error" in text_lower:
        status = "ERROR"

    return job_id, status

# 1. Login
print(f"Enter App Password for {EMAIL_USER}:")
#EMAIL_PASS = getpass.getpass()

try:
    mail = connect_to_mail(EMAIL_PROVIDER, EMAIL_USER, "")#EMAIL_PASS)
    mail.select("INBOX")

    # 2. Search
    status, messages = mail.search(None, SEARCH_CRITERIA)
    email_ids = messages[0].split()
    print(f"Found {len(email_ids)} NEOS emails.\n")

    # 3. Process latest 5 emails
    for i in email_ids[-8:]:
        res, msg_data = mail.fetch(i, "(RFC822)")
        for response_part in msg_data:
            if isinstance(response_part, tuple):
                msg = email.message_from_bytes(response_part[1])

                # Extract Text Body
                body = ""
                if msg.is_multipart():
                    for part in msg.walk():
                        if part.get_content_type() == "text/plain":
                            body = part.get_payload(decode=True).decode()
                            break
                else:
                    body = msg.get_payload(decode=True).decode()

                # Extract Data
                job_id, job_status = extract_neos_data(body)

                # Output
                print(f"Job ID: {job_id}")
                print(f"Status: {job_status}")
                print("-" * 20)

    mail.close()
    mail.logout()

except Exception as e:
    print(f"Error: {e}")

import imaplib
import email
import getpass
import re
import xmlrpc.client
import os
from google.colab import drive

# --- CONFIGURATION ---
EMAIL_USER = 'your_email@gmail.com'
EMAIL_PROVIDER = 'gmail'  # or 'yahoo'
NEOS_HOST = "https://neos-server.org:3333"
# The correct sender for job results notifications:
SEARCH_CRITERIA = '(FROM "noreply@neos.org")'
# ---------------------

# 1. Mount Google Drive (Required to save the file)
drive.mount('/content/drive')

def connect_to_mail(provider, user, password):
    host = "imap.gmail.com" if provider == 'gmail' else "imap.mail.yahoo.com"
    mail = imaplib.IMAP4_SSL(host)
    # The .strip() is crucial to avoid hidden characters from the App Password input
    mail.login(user, password.strip())
    return mail

def get_neos_credentials(text):
    # Precise regex based on your example: "Job Number=12345, Password=XYZ"
    id_match = re.search(r"Job Number=(\d+)", text)
    pass_match = re.search(r"Password=(\S+)", text)

    return (id_match.group(1) if id_match else None,
            pass_match.group(1) if pass_match else None)

# 2. Setup NEOS Client
neos = xmlrpc.client.ServerProxy(NEOS_HOST)

# 3. Email Login
print(f"Enter App Password for {EMAIL_USER} (The 16-char code):")
# Use getpass to securely input the password
EMAIL_PASS = getpass.getpass()

try:
    mail = connect_to_mail(EMAIL_PROVIDER, EMAIL_USER, EMAIL_PASS)
    mail.select("INBOX")

    # 4. Search for NEOS results emails
    status, messages = mail.search(None, SEARCH_CRITERIA)
    email_ids = messages[0].split()

    print(f"Found {len(email_ids)} result emails from noreply@neos.org.")

    # Process latest 3 emails (e.g., in case you have multiple jobs running)
    for i in email_ids[-3:]:
        res, msg_data = mail.fetch(i, "(RFC822)")
        msg = email.message_from_bytes(msg_data[0][1])

        # Extract Text Body
        body = msg.get_payload(decode=True).decode() if not msg.is_multipart() else \
               next(p.get_payload(decode=True).decode() for p in msg.walk() if p.get_content_type() == "text/plain")

        job_id, job_pass = get_neos_credentials(body)

        if job_id and job_pass:
            print(f"\n--- Processing Job {job_id} ---")

            # The email IS the "Done" status for large files. Proceed to download.
            try:
                # 5. Fetch Final Results from NEOS Server
                final_results = neos.getFinalResults(job_id, job_pass).data.decode()

                # 6. Save to Google Drive
                output_path = f'/content/drive/MyDrive/neos_results/job_{job_id}_results.txt'
                os.makedirs(os.path.dirname(output_path), exist_ok=True)

                with open(output_path, 'w') as f:
                    f.write(final_results)

                print(f"âœ… Downloaded and saved successfully to: {output_path}")

            except Exception as neos_e:
                 # Catches errors like "Job not found" or "Password incorrect"
                 print(f"âš ï¸ NEOS API Error for Job {job_id}: {neos_e}")
        else:
            print("Could not find Job Number or Password in the email body.")

    mail.close()
    mail.logout()

except Exception as e:
    print(f"\nFATAL ERROR: {e}")

"odmm dswr yobj ciob" # colab password for gmail

import imaplib

# REPLACE THESE TWO VALUES
user = 'rwsinnovations@gmail.com'
# Paste the 16-char code directly here, inside quotes, no spaces
password = "odmm dswr yobj ciob"

try:
    print(f"Attempting connection for {user}...")
    mail = imaplib.IMAP4_SSL("imap.gmail.com")
    mail.login(user, password)
    print("SUCCESS! The password works. The issue was the input method.")
    mail.logout()
except Exception as e:
    print(f"STILL FAILING: {e}")

import imaplib
import email
import getpass
import re
import xmlrpc.client
import os
from google.colab import drive

# --- CONFIGURATION ---
EMAIL_USER = 'rwsinnovations@@gmail.com'
EMAIL_PROVIDER = 'gmail'  # or 'yahoo'
EMAIL_PASS="odmm dswr yobj ciob"
NEOS_HOST = "https://neos-server.org:3333"
# The correct sender for job results notifications:
SEARCH_CRITERIA = '(FROM "no-reply@neos-server.org")'
# ---------------------

# 1. Mount Google Drive (Required to save the file)
drive.mount('/content/drive')

def connect_to_mail(provider, user, password):
    host = "imap.gmail.com" if provider == 'gmail' else "imap.mail.yahoo.com"
    mail = imaplib.IMAP4_SSL(host)
    # The .strip() is crucial to avoid hidden characters from the App Password input
    mail.login(user, password.strip())
    return mail

def get_neos_credentials(text):
    # Precise regex based on your example: "Job Number=12345, Password=XYZ"
    id_match = re.search(r"Job Number=(\d+)", text)
    pass_match = re.search(r"Password=(\S+)", text)

    return (id_match.group(1) if id_match else None,
            pass_match.group(1) if pass_match else None)

# 2. Setup NEOS Client
neos = xmlrpc.client.ServerProxy(NEOS_HOST)

# 3. Email Login
print(f"Enter App Password for {EMAIL_USER} (The 16-char code):")
# Use getpass to securely input the password
#EMAIL_PASS = getpass.getpass()

try:
    mail = connect_to_mail(EMAIL_PROVIDER, EMAIL_USER, EMAIL_PASS)
    mail.select("INBOX")

    # 4. Search for NEOS results emails
    status, messages = mail.search(None, SEARCH_CRITERIA)
    email_ids = messages[0].split()

    print(f"Found {len(email_ids)} result emails from noreply@neos.org.")

    # Process latest 3 emails (e.g., in case you have multiple jobs running)
    for i in email_ids[-8:]:
        res, msg_data = mail.fetch(i, "(RFC822)")
        msg = email.message_from_bytes(msg_data[0][1])

        # Extract Text Body
        body = msg.get_payload(decode=True).decode() if not msg.is_multipart() else \
               next(p.get_payload(decode=True).decode() for p in msg.walk() if p.get_content_type() == "text/plain")

        job_id, job_pass = get_neos_credentials(body)

        if job_id and job_pass:
            print(f"\n--- Processing Job {job_id} ---")

            # The email IS the "Done" status for large files. Proceed to download.
            try:
                # 5. Fetch Final Results from NEOS Server
                final_results = neos.getFinalResults(job_id, job_pass).data.decode()

                # 6. Save to Google Drive
                output_path = f'/content/drive/MyDrive/neos_results/job_{job_id}_results.txt'
                os.makedirs(os.path.dirname(output_path), exist_ok=True)

                with open(output_path, 'w') as f:
                    f.write(final_results)

                print(f"âœ… Downloaded and saved successfully to: {output_path}")

            except Exception as neos_e:
                 # Catches errors like "Job not found" or "Password incorrect"
                 print(f"âš ï¸ NEOS API Error for Job {job_id}: {neos_e}")
        else:
            print("Could not find Job Number or Password in the email body.")

    mail.close()
    mail.logout()

except Exception as e:
    print(f"\nFATAL ERROR: {e}")

# 1. Install the specific library first
!pip install google_colab_selenium

import google_colab_selenium as gs
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import imaplib
import email
import getpass
import re
import os
from google.colab import drive

# --- CONFIGURATION ---
EMAIL_USER = 'rwsinnovations@gmail.com'
EMAIL_PROVIDER = 'gmail'
EMAIL_PASS="odmm dswr yobj ciob"
SEARCH_CRITERIA = '(FROM "no-reply@neos-server.org")'
DRIVE_FOLDER = '/content/drive/MyDrive/neos_results'
# ---------------------

drive.mount('/content/drive')

def connect_to_mail(provider, user, password):
    host = "imap.gmail.com" if provider == 'gmail' else "imap.mail.yahoo.com"
    mail = imaplib.IMAP4_SSL(host)
    mail.login(user, password.strip())
    return mail

def get_neos_credentials(text):
    # Regex for your specific email format
    id_match = re.search(r"Job Number=(\d+)", text)
    pass_match = re.search(r"Password=(\S+)", text)
    return (id_match.group(1) if id_match else None,
            pass_match.group(1) if pass_match else None)

def fetch_result_from_web(driver, job_id, password):
    url = "http://neos-server.org/neos/admin.html"
    print(f"   âž¤ Browsing: {url}")
    driver.get(url)

    wait = WebDriverWait(driver, 15)

    try:
        # 1. Fill Job Number (name="jobnumber" is correct)
        job_input = wait.until(EC.visibility_of_element_located((By.NAME, "jobnumber")))
        job_input.clear()
        job_input.send_keys(job_id)

        # 2. Fill Password (The CORRECT name is "pass", NOT "password")
        pass_input = wait.until(EC.visibility_of_element_located((By.NAME, "pass")))
        pass_input.clear()
        pass_input.send_keys(password)

        # 3. SELECT the "View Job Results" radio button (name="admin", value="results")
        results_radio = driver.find_element(By.ID, "admin_results") # Easier to click by ID
        results_radio.click()

        # 4. Click the generic "Submit" button
        # The form has a single submit button with value="Submit"
        submit_btn = driver.find_element(By.CSS_SELECTOR, "input[type='submit'][value='Submit']")
        submit_btn.click()

        # 5. Wait for results (<pre> tag)
        print("   âž¤ Waiting for results page to load...")

        results_element = wait.until(EC.presence_of_element_located((By.TAG_NAME, "pre")))
        return results_element.text

    except Exception as e:
        # Debugging output remains for final safety check
        print(f"\n   âš ï¸ FINAL ERROR on page: {driver.title}")
        print(f"   âš ï¸ Could not interact with form elements.")
        raise e
        # --- MAIN EXECUTION ---

# 1. Login to Email
print(f"Enter App Password for {EMAIL_USER}:")
#EMAIL_PASS = getpass.getpass()

try:
    mail = connect_to_mail(EMAIL_PROVIDER, EMAIL_USER, EMAIL_PASS)
    mail.select("INBOX")

    status, messages = mail.search(None, SEARCH_CRITERIA)
    email_ids = messages[0].split()
    print(f"Found {len(email_ids)} result emails.")

    if email_ids:
        # 2. Start Driver (Only once, to save time)
        print("Starting Chrome Driver...")
        driver = gs.Chrome()

        try:
            # Process latest 3 emails
            for i in email_ids[-3:]:
                res, msg_data = mail.fetch(i, "(RFC822)")
                msg = email.message_from_bytes(msg_data[0][1])

                body = msg.get_payload(decode=True).decode() if not msg.is_multipart() else \
                       next(p.get_payload(decode=True).decode() for p in msg.walk() if p.get_content_type() == "text/plain")

                job_id, job_pass = get_neos_credentials(body)

                if job_id and job_pass:
                    print(f"\n--- Processing Job {job_id} ---")

                    # 3. Fetch from Web
                    results_text = fetch_result_from_web(driver, job_id, job_pass)

                    # 4. Save to Drive
                    output_path = f'{DRIVE_FOLDER}/job_{job_id}.txt'
                    os.makedirs(os.path.dirname(output_path), exist_ok=True)

                    with open(output_path, 'w') as f:
                        f.write(results_text)

                    print(f"   âœ“ Saved to: {output_path}")

        finally:
            driver.quit()
            print("\nDriver closed.")
    else:
        print("No emails found.")

    mail.close()
    mail.logout()

except Exception as e:
    print(f"Error: {e}")

